{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlowTron_Transfer_Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFeX9wO9iRLA"
      },
      "source": [
        "# Install dependancies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZfE7HqJiTVm"
      },
      "source": [
        "!pip install torchtext==0.7\n",
        "!pip install -q torch==1.6.0 torchvision\n",
        "!pip install unidecode==1.0.22\n",
        "!pip install tensorboardX\n",
        " import torch\n",
        " torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9XaKmWAjCgV",
        "outputId": "bb1e022c-44b5-414e-bd6e-760f0fd38a00"
      },
      "source": [
        "import sys\n",
        "\n",
        "!{sys.executable} -m pip install numpy==1.13.3 tensorflow==1.15 inflect==0.2.5 librosa==0.6.0 scipy==1.0.0 tensorboardX==1.1 Unidecode==1.0.22 pillow \n",
        "\n",
        "!git clone https://github.com/karkirowle/flowtron.git\n",
        "%cd flowtron\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "%cd tacotron2\n",
        "!git submodule update --init\n",
        "%cd ..\n",
        "\n",
        "\n",
        "!ls\n",
        "# This is ported from https://github.com/yhgon/mellotron/blob/master/inference_colab.ipynb\n",
        "!wget -N  -q https://raw.githubusercontent.com/yhgon/colab_utils/master/gfile.py\n",
        "!python gfile.py -u 'https://drive.google.com/open?id=1KhJcPawFgmfvwV7tQAOeC253rYstLrs8' -f 'flowtron_libritts.pt'\n",
        "!python gfile.py -u 'https://drive.google.com/open?id=1Cjd6dK_eFz6DE0PKXKgKxrzTUqzzUDW-' -f 'flowtron_ljs.pt'\n",
        "\n",
        "!python gfile.py -u 'https://drive.google.com/open?id=1Rm5rV5XaWWiUbIpg5385l5sh68z2bVOE' -f 'waveglow_256channels_v4.pt'\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/2d/005e45738ab07a26e621c9c12dc97381f372e06678adf7dc3356a69b5960/numpy-1.13.3.zip (5.0MB)\n",
            "\u001b[K     |████████████████████████████████| 5.0MB 2.2MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hCollecting inflect==0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/15/2d176749884cbeda0c92e0d09e1303ff53a973eb3c6bb2136803b9d962c9/inflect-0.2.5-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n",
            "\u001b[?25hCollecting librosa==0.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6b/f4/422bfbefd581f74354ef05176aa48558c548243c87e359d91512d4b65523/librosa-0.6.0.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 23.4MB/s \n",
            "\u001b[?25hCollecting scipy==1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/73/76fc6ea21818eed0de8dd38e1e9586725578864169a2b31acdeffb9131c8/scipy-1.0.0.tar.gz (15.2MB)\n",
            "\u001b[K     |████████████████████████████████| 15.2MB 339kB/s \n",
            "\u001b[?25hCollecting tensorboardX==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0a/95/b5eb020dcda7568cafd9a035f61bffa38570995a2a7d024283513a230406/tensorboardX-1.1-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hCollecting Unidecode==1.0.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (7.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.34.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.36.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 47.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (2.1.9)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (1.0.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from librosa==0.6.0) (0.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (57.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: numba>=0.32 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.0->librosa==0.6.0) (0.51.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (4.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15) (1.5.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.32->resampy>=0.2.0->librosa==0.6.0) (0.34.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.1)\n",
            "Building wheels for collected packages: numpy, librosa, scipy, gast\n",
            "  Building wheel for numpy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for numpy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for numpy\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for numpy\u001b[0m\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.6.0-cp37-none-any.whl size=1553496 sha256=5dc0bf0416695b2fdb384f2557e09ef04316a983b68c255d269ee34a4f79b97e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/19/fa/71097e2207df1cc613749f15b2f0b1972c167b36d6afc09d15\n",
            "  Building wheel for scipy (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for scipy\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for scipy\n",
            "\u001b[31m  ERROR: Failed cleaning build dir for scipy\u001b[0m\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=a4f04db6f4da2c2e6cbcb51dbd90cf904373026fc68115a306b93dfdc400e68a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built librosa gast\n",
            "Failed to build numpy scipy\n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tifffile 2021.4.8 has requirement numpy>=1.15.1, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: seaborn 0.11.1 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pymc3 3.11.2 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement scipy>=1.2.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pandas 1.1.5 has requirement numpy>=1.15.4, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: opencv-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: opencv-contrib-python 4.1.2.30 has requirement numpy>=1.14.5, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.51.2 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement librosa>=0.7.2, but you'll have librosa 0.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: jaxlib 0.1.66+cuda110 has requirement numpy>=1.16, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: h5py 3.1.0 has requirement numpy>=1.14.5; python_version == \"3.7\", but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement numpy>=1.15.4, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.61 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement numpy>=1.15, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.31 has requirement scipy>=1.1.0, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.13.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, tensorboard, keras-applications, gast, tensorflow-estimator, tensorflow, inflect, scipy, librosa, tensorboardX, Unidecode\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "    Running setup.py install for numpy ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of numpy\n",
            "  Moving to /usr/bin/f2py\n",
            "   from /tmp/pip-uninstall-t_nfdjw9/f2py\n",
            "  Moving to /usr/local/bin/f2py\n",
            "   from /tmp/pip-uninstall-ld_0lphf/f2py\n",
            "  Moving to /usr/local/bin/f2py3\n",
            "   from /tmp/pip-uninstall-ld_0lphf/f2py3\n",
            "  Moving to /usr/local/bin/f2py3.7\n",
            "   from /tmp/pip-uninstall-ld_0lphf/f2py3.7\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy-1.19.5.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy-1.19.5.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy.libs/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy.libs\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/numpy/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~umpy\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-l89xmubv/numpy/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-l89xmubv/numpy/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-toiml9lh/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n",
            "Cloning into 'flowtron'...\n",
            "remote: Enumerating objects: 108, done.\u001b[K\n",
            "remote: Total 108 (delta 0), reused 0 (delta 0), pack-reused 108\u001b[K\n",
            "Receiving objects: 100% (108/108), 2.54 MiB | 4.06 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/flowtron\n",
            "Submodule 'apex' (https://github.com/NVIDIA/apex) registered for path 'apex'\n",
            "Submodule 'tacotron2' (https://github.com/NVIDIA/tacotron2) registered for path 'tacotron2'\n",
            "Cloning into '/content/flowtron/apex'...\n",
            "Cloning into '/content/flowtron/tacotron2'...\n",
            "Submodule path 'apex': checked out '9165b27fdf240f9bc08eac98b849a9d7c6308917'\n",
            "Submodule path 'tacotron2': checked out '6f435f7f29c3e1553cf2dd7ca2daf56903b20c39'\n",
            "/content/flowtron/tacotron2\n",
            "Submodule 'waveglow' (https://github.com/NVIDIA/waveglow) registered for path 'waveglow'\n",
            "Cloning into '/content/flowtron/tacotron2/waveglow'...\n",
            "Submodule path 'waveglow': checked out '2fd4e63e2918012f55eac2c8a8e75622a39741be'\n",
            "/content/flowtron\n",
            "apex\t\t\t    flowtron.py\n",
            "audio_processing.py\t    inference.py\n",
            "config.json\t\t    LICENSE\n",
            "data\t\t\t    README.md\n",
            "data.py\t\t\t    requirements.txt\n",
            "distributed.py\t\t    Style_Transfer_for_Flowtron.ipynb\n",
            "filelists\t\t    tacotron2\n",
            "flowtron_logger.py\t    text\n",
            "flowtron_plotting_utils.py  train.py\n",
            "It took  2.73sec to download 232.7 MB flowtron_libritts.pt \n",
            "It took  2.67sec to download 232.6 MB flowtron_ljs.pt \n",
            "It took  25.64sec to download 1.3 GB waveglow_256channels_v4.pt \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQti721dlF8i"
      },
      "source": [
        "# Make alterations to Flowtron files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAs19Aesk4Jk"
      },
      "source": [
        "### Alter the train.py file with the following code\n",
        "```\n",
        "###########################################################################\n",
        "#\n",
        "#  Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "#\n",
        "###########################################################################\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import ast\n",
        "\n",
        "from flowtron import FlowtronLoss\n",
        "from flowtron import Flowtron\n",
        "from data import Data, DataCollate\n",
        "from flowtron_logger import FlowtronLogger\n",
        "\n",
        "\n",
        "#=====START: ADDED FOR DISTRIBUTED======\n",
        "from distributed import init_distributed, apply_gradient_allreduce, reduce_tensor\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "#=====END:   ADDED FOR DISTRIBUTED======\n",
        "\n",
        "def update_params(config, params):\n",
        "    for param in params:\n",
        "        print(param)\n",
        "        k, v = param.split(\"=\")\n",
        "        try:\n",
        "            v = ast.literal_eval(v)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        k_split = k.split('.')\n",
        "        if len(k_split) > 1:\n",
        "            parent_k = k_split[0]\n",
        "            cur_param = ['.'.join(k_split[1:])+\"=\"+str(v)]\n",
        "            update_params(config[parent_k], cur_param)\n",
        "        elif k in config and len(k_split) == 1:\n",
        "            config[k] = v\n",
        "        else:\n",
        "            print(\"{}, {} params not updated\".format(k, v))\n",
        "\n",
        "\n",
        "def prepare_dataloaders(data_config, n_gpus, batch_size):\n",
        "    # Get data, data loaders and 1ollate function ready\n",
        "    ignore_keys = ['training_files', 'validation_files']\n",
        "    trainset = Data(data_config['training_files'],\n",
        "                    **dict((k, v) for k, v in data_config.items()\n",
        "                    if k not in ignore_keys))\n",
        "\n",
        "    valset = Data(data_config['validation_files'],\n",
        "                  **dict((k, v) for k, v in data_config.items()\n",
        "                  if k not in ignore_keys), speaker_ids=trainset.speaker_ids)\n",
        "\n",
        "    collate_fn = DataCollate()\n",
        "\n",
        "    train_sampler, shuffle = None, True\n",
        "    if n_gpus > 1:\n",
        "        train_sampler, shuffle = DistributedSampler(trainset), False\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler, batch_size=batch_size,\n",
        "                              pin_memory=False, drop_last=True,\n",
        "                              collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def warmstart(checkpoint_path, model, include_layers=None):\n",
        "    print(\"Warm starting model\", checkpoint_path)\n",
        "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    if 'model' in pretrained_dict:\n",
        "        pretrained_dict = pretrained_dict['model'].state_dict()\n",
        "    else:\n",
        "        pretrained_dict = pretrained_dict['state_dict']\n",
        "\n",
        "    if include_layers is not None:\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                           if any(l in k for l in include_layers)}\n",
        "\n",
        "    model_dict = model.state_dict()\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items()\n",
        "                       if k in model_dict}\n",
        "\n",
        "    if pretrained_dict['speaker_embedding.weight'].shape != model_dict['speaker_embedding.weight'].shape:\n",
        "        del pretrained_dict['speaker_embedding.weight']\n",
        "\n",
        "    model_dict.update(pretrained_dict)\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer, ignore_layers=[]):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    #iteration = checkpoint_dict['iteration']\n",
        "    #model_dict = checkpoint_dict['model'].state_dict()\n",
        "    if 'state_dict' in checkpoint_dict:\n",
        "        print(\"active state\")\n",
        "        state_dict = checkpoint_dict['state_dict']\n",
        "        model.load_state_dict(state_dict)\n",
        "    elif 'model' in checkpoint_dict:\n",
        "        print(\"active model\")\n",
        "        model = checkpoint_dict['model'].cuda()\n",
        "    else:\n",
        "        assert False, \"cannot load model!\"\n",
        "    if len(ignore_layers) > 0:\n",
        "        state_dict = {k: v for k, v in state_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(state_dict)\n",
        "        state_dict = dummy_dict\n",
        "    else:\n",
        "        optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    print(\"Loaded checkpoint '{}' (iteration )\" .format(\n",
        "          checkpoint_path))\n",
        "    return model, optimizer\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "          iteration, filepath))\n",
        "    model_for_saving = Flowtron(**model_config).cuda()\n",
        "    model_for_saving.load_state_dict(model.state_dict())\n",
        "    torch.save({'model': model_for_saving,\n",
        "                'iteration': iteration,\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'learning_rate': learning_rate}, filepath)\n",
        "\n",
        "\n",
        "def compute_validation_loss(model, criterion, valset, collate_fn, batch_size,\n",
        "                            n_gpus):                     \n",
        "    model.eval()\n",
        "    print(\"eval\")  \n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if n_gpus > 1 else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "        print(\"next iter\")\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            print(\"next 1\")\n",
        "            mel, speaker_vecs, text, in_lens, out_lens, gate_target = batch\n",
        "            print(\"next 2\")\n",
        "            mel, speaker_vecs, text = mel.cuda(), speaker_vecs.cuda(), text.cuda()\n",
        "            print(\"next 3\")\n",
        "            in_lens, out_lens, gate_target = in_lens.cuda(), out_lens.cuda(), gate_target.cuda()\n",
        "            z, log_s_list, gate_pred, attn, mean, log_var, prob = model(\n",
        "                mel, speaker_vecs, text, in_lens, out_lens)\n",
        "\n",
        "            loss = criterion((z, log_s_list, gate_pred, mean, log_var, prob),\n",
        "                             gate_target, out_lens)\n",
        "\n",
        "            if n_gpus > 1:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        print(\"end\")\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    print(\"here\")\n",
        "    print(\"Mean {}\\nLogVar {}\\nProb {}\".format(mean, log_var, prob))\n",
        "    model.train()\n",
        "    return val_loss, attn, gate_pred, gate_target\n",
        "\n",
        "\n",
        "def train(n_gpus, rank, output_directory, epochs, learning_rate, weight_decay,\n",
        "          sigma, iters_per_checkpoint, batch_size, seed, checkpoint_path,\n",
        "          ignore_layers, include_layers, warmstart_checkpoint_path,\n",
        "          with_tensorboard, fp16_run):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    if n_gpus > 1:\n",
        "        init_distributed(rank, n_gpus, **dist_config)\n",
        "\n",
        "    criterion = FlowtronLoss(sigma, model_config['n_components'] > 1,\n",
        "                             model_config['use_gate_layer'])\n",
        "    model = Flowtron(**model_config).cuda()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=weight_decay)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    if warmstart_checkpoint_path != \"\":\n",
        "        model = warmstart(warmstart_checkpoint_path, model)\n",
        "\n",
        "    if checkpoint_path != \"\":\n",
        "        model, optimizer = load_checkpoint(checkpoint_path, model,\n",
        "                                                      optimizer, ignore_layers)\n",
        "        iteration += 1  # next iteration is iteration + 1\n",
        "\n",
        "    if n_gpus > 1:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "    print(model)\n",
        "    if fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(\n",
        "        data_config, n_gpus, batch_size)\n",
        "\n",
        "    # Get shared output_directory ready\n",
        "    if rank == 0 and not os.path.isdir(output_directory):\n",
        "        os.makedirs(output_directory)\n",
        "        os.chmod(output_directory, 0o775)\n",
        "    print(\"output directory\", output_directory)\n",
        "\n",
        "    if with_tensorboard and rank == 0:\n",
        "        logger = FlowtronLogger(os.path.join(output_directory, 'logs'))\n",
        "\n",
        "    model.train()\n",
        "    epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in range(epoch_offset, epochs):\n",
        "        print(\"Epoch: {}\".format(epoch))\n",
        "        for batch in train_loader:\n",
        "            model.zero_grad()\n",
        "\n",
        "            mel, speaker_vecs, text, in_lens, out_lens, gate_target = batch\n",
        "            mel, speaker_vecs, text = mel.cuda(), speaker_vecs.cuda(), text.cuda()\n",
        "            in_lens, out_lens, gate_target = in_lens.cuda(), out_lens.cuda(), gate_target.cuda()\n",
        "\n",
        "            z, log_s_list, gate_pred, attn, mean, log_var, prob = model(\n",
        "                mel, speaker_vecs, text, in_lens, out_lens)\n",
        "            loss = criterion((z, log_s_list, gate_pred, mean, log_var, prob),\n",
        "                             gate_target, out_lens)\n",
        "\n",
        "            if n_gpus > 1:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "\n",
        "            if fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if rank == 0:\n",
        "                print(\"{}:\\t{:.9f}\".format(iteration, reduced_loss), flush=True)\n",
        "\n",
        "            if with_tensorboard and rank == 0:\n",
        "                logger.add_scalar('training_loss', reduced_loss, iteration)\n",
        "                logger.add_scalar('learning_rate', learning_rate, iteration)\n",
        "\n",
        "            if (iteration % iters_per_checkpoint == 0):\n",
        "                print(\"checkpoint time\")\n",
        "                #val_loss, attns, gate_pred, gate_target = compute_validation_loss(\n",
        "                  #  model, criterion, valset, collate_fn, batch_size, n_gpus)\n",
        "                if rank == 0:\n",
        "                    #print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))\n",
        "                    #if with_tensorboard:\n",
        "                    #    logger.log_validation(\n",
        "                    #        val_loss, attns, gate_pred, gate_target, iteration)\n",
        "\n",
        "                    checkpoint_path = \"{}/model_{}\".format(\n",
        "                        output_directory, iteration)\n",
        "                    save_checkpoint(model, optimizer, learning_rate, iteration,\n",
        "                                    checkpoint_path)\n",
        "\n",
        "            iteration += 1\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-c', '--config', type=str,\n",
        "                        help='JSON file for configuration')\n",
        "    parser.add_argument('-p', '--params', nargs='+', default=[])\n",
        "    args = parser.parse_args()\n",
        "    args.rank = 0\n",
        "\n",
        "    # Parse configs.  Globals nicer in this case\n",
        "    with open(args.config) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global config\n",
        "    config = json.loads(data)\n",
        "    update_params(config, args.params)\n",
        "    print(config)\n",
        "\n",
        "    train_config = config[\"train_config\"]\n",
        "    global data_config\n",
        "    data_config = config[\"data_config\"]\n",
        "    global dist_config\n",
        "    dist_config = config[\"dist_config\"]\n",
        "    global model_config\n",
        "    model_config = config[\"model_config\"]\n",
        "\n",
        "    # Make sure the launcher sets `RANK` and `WORLD_SIZE`.\n",
        "    rank = int(os.getenv('RANK', '0'))\n",
        "    n_gpus = int(os.getenv(\"WORLD_SIZE\", '1'))\n",
        "    print('> got rank {} and world size {} ...'.format(rank, n_gpus))\n",
        "\n",
        "    if n_gpus == 1 and rank != 0:\n",
        "        raise Exception(\"Doing single GPU training on rank > 0\")\n",
        "\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    train(n_gpus, rank, **train_config)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgDKRgrEp162"
      },
      "source": [
        "### Alter the Inferennce.py file with the following code\n",
        "```\n",
        "###############################################################################\n",
        "#\n",
        "#  Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\n",
        "#  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "#  you may not use this file except in compliance with the License.\n",
        "#  You may obtain a copy of the License at\n",
        "#\n",
        "#      http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "#  Unless required by applicable law or agreed to in writing, software\n",
        "#  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "#  See the License for the specific language governing permissions and\n",
        "#  limitations under the License.\n",
        "#\n",
        "###############################################################################\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import sys\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "from flowtron import Flowtron\n",
        "from torch.utils.data import DataLoader\n",
        "from data import Data\n",
        "from train import update_params\n",
        "\n",
        "sys.path.insert(0, \"tacotron2\")\n",
        "sys.path.insert(0, \"tacotron2/waveglow\")\n",
        "from glow import WaveGlow\n",
        "from scipy.io.wavfile import write\n",
        "\n",
        "\n",
        "def infer(flowtron_path, waveglow_path, output_dir, text, speaker_id, n_frames,\n",
        "          sigma, gate_threshold, seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # load waveglow\n",
        "    waveglow = torch.load(waveglow_path)['model'].cuda().eval()\n",
        "    waveglow.cuda().half()\n",
        "    for k in waveglow.convinv:\n",
        "        k.float()\n",
        "    waveglow.eval()\n",
        "\n",
        "    # load flowtron\n",
        "    model = Flowtron(**model_config).cuda()\n",
        "    state_dict = torch.load(flowtron_path, map_location='cpu')['model'].cuda()\n",
        "    #model.load_state_dict(state_dict)\n",
        "\n",
        "    model.eval()\n",
        "    print(\"Loaded checkpoint '{}')\" .format(flowtron_path))\n",
        "\n",
        "    ignore_keys = ['training_files', 'validation_files']\n",
        "    trainset = Data(\n",
        "        data_config['training_files'],\n",
        "        **dict((k, v) for k, v in data_config.items() if k not in ignore_keys))\n",
        "    speaker_vecs = trainset.get_speaker_id(speaker_id).cuda()\n",
        "    text = trainset.get_text(text).cuda()\n",
        "    speaker_vecs = speaker_vecs[None]\n",
        "    text = text[None]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        residual = torch.cuda.FloatTensor(1, 80, n_frames).normal_() * sigma\n",
        "        mels, attentions = model.infer(\n",
        "            residual, speaker_vecs, text, gate_threshold=gate_threshold)\n",
        "\n",
        "    for k in range(len(attentions)):\n",
        "        attention = torch.cat(attentions[k]).cpu().numpy()\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "        axes[0].imshow(mels[0].cpu().numpy(), origin='bottom', aspect='auto')\n",
        "        axes[1].imshow(attention[:, 0].transpose(), origin='bottom', aspect='auto')\n",
        "        fig.savefig(os.path.join(output_dir, 'sid{}_sigma{}_attnlayer{}.png'.format(speaker_id, sigma, k)))\n",
        "        plt.close(\"all\")\n",
        "\n",
        "    audio = waveglow.infer(mels.half(), sigma=0.8).float()\n",
        "    audio = audio.cpu().numpy()[0]\n",
        "    # normalize audio for now\n",
        "    audio = audio / np.abs(audio).max()\n",
        "    print(audio.shape)\n",
        "\n",
        "    write(os.path.join(output_dir, 'sid{}_sigma{}.wav'.format(speaker_id, sigma)),\n",
        "          data_config['sampling_rate'], audio)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('-c', '--config', type=str,\n",
        "                        help='JSON file for configuration')\n",
        "    parser.add_argument('-p', '--params', nargs='+', default=[])\n",
        "    parser.add_argument('-f', '--flowtron_path',\n",
        "                        help='Path to flowtron state dict', type=str)\n",
        "    parser.add_argument('-w', '--waveglow_path',\n",
        "                        help='Path to waveglow state dict', type=str)\n",
        "    parser.add_argument('-t', '--text', help='Text to synthesize', type=str)\n",
        "    parser.add_argument('-i', '--id', help='Speaker id', type=int)\n",
        "    parser.add_argument('-n', '--n_frames', help='Number of frames',\n",
        "                        default=400, type=int)\n",
        "    parser.add_argument('-o', \"--output_dir\", default=\"results/\")\n",
        "    parser.add_argument(\"-s\", \"--sigma\", default=0.5, type=float)\n",
        "    parser.add_argument(\"-g\", \"--gate\", default=0.5, type=float)\n",
        "    parser.add_argument(\"--seed\", default=1234, type=int)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Parse configs.  Globals nicer in this case\n",
        "    with open(args.config) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global config\n",
        "    config = json.loads(data)\n",
        "    update_params(config, args.params)\n",
        "\n",
        "    data_config = config[\"data_config\"]\n",
        "    global model_config\n",
        "    model_config = config[\"model_config\"]\n",
        "\n",
        "    # Make directory if it doesn't exist\n",
        "    if not os.path.isdir(args.output_dir):\n",
        "        os.makedirs(args.output_dir)\n",
        "        os.chmod(args.output_dir, 0o775)\n",
        "\n",
        "    torch.backends.cudnn.enabled = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    infer(args.flowtron_path, args.waveglow_path, args.output_dir, args.text,\n",
        "          args.id, args.n_frames, args.sigma, args.gate, args.seed)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZdIXc1diis9"
      },
      "source": [
        "# Download Training and Validation files\n",
        " \n",
        "Down files from https://drive.google.com/drive/folders/1QNu4Wx07oRJ9xQ6EJRYZWgoKSBw37QVS?usp=sharing and upload to files in colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd1wCinoXYK5"
      },
      "source": [
        "# add a new emotests directory\n",
        "! mkdir -p emotests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qne4Rilzi58L"
      },
      "source": [
        "# Add files to emotests directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI6Jvny5JRlg",
        "outputId": "30840a10-a582-4374-b374-89e646be5f00"
      },
      "source": [
        "!tar -xvf train.tar.xz -C emotests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train/\n",
            "train/sam_neutral_227.wav\n",
            "train/jenie_disgust_150.wav\n",
            "train/sam_amused_202.wav\n",
            "train/sam_sleepiness_236.wav\n",
            "train/jenie_neutral_151.wav\n",
            "train/jen_amused_089.wav\n",
            "train/bea_amused_014.wav\n",
            "train/bea_neutral_058.wav\n",
            "train/sam_amused_205.wav\n",
            "train/bea_amused_015.wav\n",
            "train/josh_amused_174.wav\n",
            "train/bea_disgust_035.wav\n",
            "train/bea_sleepiness_074.wav\n",
            "train/sam_sleepiness_231.wav\n",
            "train/jen_anger_119.wav\n",
            "train/jenie_sleepiness_161.wav\n",
            "train/sam_sleepiness_239.wav\n",
            "train/jen_anger_115.wav\n",
            "train/jenie_sleepiness_167.wav\n",
            "train/bea_neutral_048.wav\n",
            "train/bea_sleepiness_068.wav\n",
            "train/josh_neutral_181.wav\n",
            "train/josh_amused_172.wav\n",
            "train/bea_sleepiness_071.wav\n",
            "train/jenie_sleepiness_164.wav\n",
            "train/bea_amused_013.wav\n",
            "train/josh_amused_175.wav\n",
            "train/jen_amused_086.wav\n",
            "train/sam_disgust_212.wav\n",
            "train/sam_amused_204.wav\n",
            "train/josh_sleepiness_198.wav\n",
            "train/josh_sleepiness_192.wav\n",
            "train/bea_amused_002.wav\n",
            "train/jen_amused_084.wav\n",
            "train/jen_amused_077.wav\n",
            "train/josh_sleepiness_193.wav\n",
            "train/jen_anger_118.wav\n",
            "train/bea_anger_025.wav\n",
            "train/sam_neutral_224.wav\n",
            "train/bea_amused_004.wav\n",
            "train/bea_sleepiness_064.wav\n",
            "train/josh_sleepiness_194.wav\n",
            "train/jen_amused_078.wav\n",
            "train/bea_sleepiness_073.wav\n",
            "train/sam_disgust_220.wav\n",
            "train/jen_amused_082.wav\n",
            "train/josh_sleepiness_191.wav\n",
            "train/jenie_sleepiness_163.wav\n",
            "train/bea_sleepiness_061.wav\n",
            "train/jen_amused_081.wav\n",
            "train/bea_neutral_053.wav\n",
            "train/josh_amused_178.wav\n",
            "train/bea_anger_016.wav\n",
            "train/josh_amused_171.wav\n",
            "train/josh_neutral_182.wav\n",
            "train/josh_amused_176.wav\n",
            "train/jenie_disgust_141.wav\n",
            "train/josh_neutral_190.wav\n",
            "train/sam_disgust_216.wav\n",
            "train/jenie_neutral_160.wav\n",
            "train/bea_anger_023.wav\n",
            "train/bea_anger_029.wav\n",
            "train/sam_amused_209.wav\n",
            "train/jenie_disgust_149.wav\n",
            "train/josh_sleepiness_195.wav\n",
            "train/bea_sleepiness_062.wav\n",
            "train/jen_anger_113.wav\n",
            "train/jen_amused_088.wav\n",
            "train/sam_disgust_211.wav\n",
            "train/bea_neutral_057.wav\n",
            "train/sam_disgust_213.wav\n",
            "train/bea_disgust_034.wav\n",
            "train/sam_neutral_222.wav\n",
            "train/sam_sleepiness_233.wav\n",
            "train/jenie_sleepiness_162.wav\n",
            "train/bea_disgust_045.wav\n",
            "train/bea_neutral_046.wav\n",
            "train/jen_amused_083.wav\n",
            "train/jenie_neutral_158.wav\n",
            "train/bea_sleepiness_075.wav\n",
            "train/jen_amused_090.wav\n",
            "train/sam_neutral_225.wav\n",
            "train/sam_neutral_221.wav\n",
            "train/bea_disgust_038.wav\n",
            "train/bea_sleepiness_063.wav\n",
            "train/sam_disgust_219.wav\n",
            "train/sam_amused_206.wav\n",
            "train/jenie_neutral_153.wav\n",
            "train/josh_sleepiness_200.wav\n",
            "train/jenie_sleepiness_166.wav\n",
            "train/jenie_disgust_142.wav\n",
            "train/bea_disgust_032.wav\n",
            "train/bea_disgust_031.wav\n",
            "train/bea_neutral_059.wav\n",
            "train/jenie_sleepiness_170.wav\n",
            "train/josh_amused_173.wav\n",
            "train/jenie_neutral_159.wav\n",
            "train/bea_sleepiness_072.wav\n",
            "train/jenie_neutral_157.wav\n",
            "train/jen_anger_116.wav\n",
            "train/bea_sleepiness_070.wav\n",
            "train/bea_anger_026.wav\n",
            "train/josh_sleepiness_199.wav\n",
            "train/bea_anger_024.wav\n",
            "train/bea_sleepiness_066.wav\n",
            "train/josh_neutral_184.wav\n",
            "train/bea_disgust_033.wav\n",
            "train/bea_anger_018.wav\n",
            "train/jenie_disgust_148.wav\n",
            "train/sam_neutral_230.wav\n",
            "train/bea_neutral_054.wav\n",
            "train/jen_amused_076.wav\n",
            "train/jenie_disgust_147.wav\n",
            "train/sam_sleepiness_237.wav\n",
            "train/sam_amused_201.wav\n",
            "train/jen_anger_117.wav\n",
            "train/bea_neutral_056.wav\n",
            "train/jenie_sleepiness_168.wav\n",
            "train/sam_amused_208.wav\n",
            "train/bea_anger_028.wav\n",
            "train/bea_disgust_041.wav\n",
            "train/bea_disgust_037.wav\n",
            "train/sam_neutral_226.wav\n",
            "train/josh_neutral_185.wav\n",
            "train/jen_anger_114.wav\n",
            "train/bea_disgust_036.wav\n",
            "train/sam_amused_207.wav\n",
            "train/sam_sleepiness_232.wav\n",
            "train/josh_neutral_187.wav\n",
            "train/bea_amused_011.wav\n",
            "train/josh_amused_179.wav\n",
            "train/sam_neutral_223.wav\n",
            "train/jenie_disgust_145.wav\n",
            "train/sam_sleepiness_240.wav\n",
            "train/bea_amused_001.wav\n",
            "train/sam_sleepiness_235.wav\n",
            "train/bea_amused_005.wav\n",
            "train/bea_anger_022.wav\n",
            "train/sam_sleepiness_238.wav\n",
            "train/jenie_sleepiness_165.wav\n",
            "train/jenie_disgust_144.wav\n",
            "train/sam_amused_210.wav\n",
            "train/bea_anger_020.wav\n",
            "train/sam_disgust_214.wav\n",
            "train/bea_amused_010.wav\n",
            "train/bea_amused_012.wav\n",
            "train/bea_amused_009.wav\n",
            "train/bea_disgust_043.wav\n",
            "train/bea_disgust_044.wav\n",
            "train/josh_amused_180.wav\n",
            "train/bea_anger_017.wav\n",
            "train/bea_anger_019.wav\n",
            "train/jen_amused_080.wav\n",
            "train/sam_neutral_229.wav\n",
            "train/bea_neutral_051.wav\n",
            "train/bea_sleepiness_069.wav\n",
            "train/bea_anger_027.wav\n",
            "train/josh_sleepiness_196.wav\n",
            "train/jenie_neutral_155.wav\n",
            "train/josh_neutral_188.wav\n",
            "train/bea_amused_007.wav\n",
            "train/jenie_neutral_152.wav\n",
            "train/josh_neutral_189.wav\n",
            "train/bea_disgust_040.wav\n",
            "train/jenie_sleepiness_169.wav\n",
            "train/jen_amused_085.wav\n",
            "train/jen_anger_120.wav\n",
            "train/sam_neutral_228.wav\n",
            "train/jen_amused_079.wav\n",
            "train/sam_disgust_215.wav\n",
            "train/sam_disgust_217.wav\n",
            "train/josh_sleepiness_197.wav\n",
            "train/bea_anger_030.wav\n",
            "train/jenie_neutral_156.wav\n",
            "train/bea_disgust_039.wav\n",
            "train/bea_anger_021.wav\n",
            "train/josh_neutral_186.wav\n",
            "train/bea_neutral_060.wav\n",
            "train/jen_amused_087.wav\n",
            "train/bea_amused_003.wav\n",
            "train/bea_neutral_055.wav\n",
            "train/jenie_disgust_143.wav\n",
            "train/bea_neutral_052.wav\n",
            "train/bea_neutral_047.wav\n",
            "train/bea_amused_006.wav\n",
            "train/bea_neutral_049.wav\n",
            "train/bea_sleepiness_065.wav\n",
            "train/bea_neutral_050.wav\n",
            "train/jenie_disgust_146.wav\n",
            "train/sam_sleepiness_234.wav\n",
            "train/bea_disgust_042.wav\n",
            "train/sam_amused_203.wav\n",
            "train/sam_disgust_218.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9jFymUvDJSBi",
        "outputId": "c24caf8f-ca0a-4630-9294-371221d11c9b"
      },
      "source": [
        "!tar -xvf /content/test.tar.xz -C /content/emotests"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test/\n",
            "test/jenie_sleepiness_501.wav\n",
            "test/jen_anger_532.wav\n",
            "test/josh_amused_305.wav\n",
            "test/jen_amused_472.wav\n",
            "test/bea_neutral_362.wav\n",
            "test/jenie_disgust_560.wav\n",
            "test/jenie_sleepiness_504.wav\n",
            "test/bea_anger_334.wav\n",
            "test/josh_neutral_306.wav\n",
            "test/jen_anger_528.wav\n",
            "test/bea_neutral_363.wav\n",
            "test/josh_sleepiness_306.wav\n",
            "test/jen_amused_474.wav\n",
            "test/jen_amused_473.wav\n",
            "test/jenie_sleepiness_503.wav\n",
            "test/josh_neutral_308.wav\n",
            "test/josh_amused_306.wav\n",
            "test/jenie_neutral_448.wav\n",
            "test/josh_sleepiness_308.wav\n",
            "test/bea_amused_304.wav\n",
            "test/bea_neutral_360.wav\n",
            "test/bea_disgust_332.wav\n",
            "test/jen_amused_476.wav\n",
            "test/bea_disgust_334.wav\n",
            "test/josh_amused_307.wav\n",
            "test/bea_disgust_336.wav\n",
            "test/jenie_disgust_559.wav\n",
            "test/bea_disgust_333.wav\n",
            "test/bea_sleepiness_501.wav\n",
            "test/josh_amused_308.wav\n",
            "test/jen_anger_531.wav\n",
            "test/bea_neutral_364.wav\n",
            "test/josh_sleepiness_307.wav\n",
            "test/bea_amused_306.wav\n",
            "test/bea_anger_336.wav\n",
            "test/bea_sleepiness_503.wav\n",
            "test/jenie_neutral_445.wav\n",
            "test/jenie_neutral_447.wav\n",
            "test/josh_neutral_307.wav\n",
            "test/jen_amused_475.wav\n",
            "test/bea_sleepiness_504.wav\n",
            "test/bea_sleepiness_500.wav\n",
            "test/jen_anger_529.wav\n",
            "test/bea_sleepiness_502.wav\n",
            "test/jen_anger_530.wav\n",
            "test/josh_neutral_305.wav\n",
            "test/jenie_disgust_557.wav\n",
            "test/bea_amused_307.wav\n",
            "test/jenie_neutral_446.wav\n",
            "test/josh_sleepiness_305.wav\n",
            "test/bea_amused_308.wav\n",
            "test/jenie_sleepiness_500.wav\n",
            "test/bea_neutral_361.wav\n",
            "test/jenie_sleepiness_502.wav\n",
            "test/josh_amused_304.wav\n",
            "test/bea_anger_335.wav\n",
            "test/jenie_disgust_558.wav\n",
            "test/bea_anger_333.wav\n",
            "test/bea_amused_305.wav\n",
            "test/bea_anger_332.wav\n",
            "test/bea_disgust_335.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sLl--eNj925"
      },
      "source": [
        "## NOTE:\n",
        "If the training files do not download properly use the following method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQJN-v_JkF0r"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg8BYAhZkCBZ"
      },
      "source": [
        "!tar -xvf /content/drive/MyDrive/filelists/train.tar.xz -C /content/emotests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxxeVpuPq5Fa"
      },
      "source": [
        "# Begin Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQk4eC4TXwdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe0000a4-0ac4-469f-a213-a5f6a862673c"
      },
      "source": [
        "!python /content/flowtron/train.py -c config.json -p train_config.ignore_layers=[\"speaker_embedding.weight\"] train_config.checkpoint_path=\"/content/flowtron/flowtron_ljs.pt\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_config.ignore_layers=[speaker_embedding.weight]\n",
            "ignore_layers=[speaker_embedding.weight]\n",
            "train_config.checkpoint_path=/content/flowtron/flowtron_ljs.pt\n",
            "checkpoint_path=/content/flowtron/flowtron_ljs.pt\n",
            "{'train_config': {'output_directory': 'outdir', 'epochs': 10000000, 'learning_rate': 0.0001, 'weight_decay': 1e-06, 'sigma': 1.0, 'iters_per_checkpoint': 500, 'batch_size': 12, 'seed': 1234, 'checkpoint_path': '/content/flowtron/flowtron_ljs.pt', 'ignore_layers': '[speaker_embedding.weight]', 'include_layers': ['speaker', 'encoder', 'embedding'], 'warmstart_checkpoint_path': '', 'with_tensorboard': True, 'fp16_run': False}, 'data_config': {'training_files': '/content/all_train_fin.txt', 'validation_files': '/content/all_val.txt', 'text_cleaners': ['flowtron_cleaners'], 'p_arpabet': 0.5, 'cmudict_path': 'data/cmudict_dictionary', 'sampling_rate': 22050, 'filter_length': 1024, 'hop_length': 256, 'win_length': 1024, 'mel_fmin': 0.0, 'mel_fmax': 8000.0, 'max_wav_value': 32768.0}, 'dist_config': {'dist_backend': 'nccl', 'dist_url': 'tcp://localhost:54321'}, 'model_config': {'n_speakers': 1, 'n_speaker_dim': 128, 'n_text': 185, 'n_text_dim': 512, 'n_flows': 2, 'n_mel_channels': 80, 'n_attn_channels': 640, 'n_hidden': 1024, 'n_lstm_layers': 2, 'mel_encoder_n_hidden': 512, 'n_components': 0, 'mean_scale': 0.0, 'fixed_gaussian': True, 'dummy_speaker_embedding': False, 'use_gate_layer': True}}\n",
            "> got rank 0 and world size 1 ...\n",
            "Loaded checkpoint '/content/flowtron/flowtron_ljs.pt' (iteration )\n",
            "Flowtron(\n",
            "  (speaker_embedding): Embedding(1, 128)\n",
            "  (embedding): Embedding(185, 512)\n",
            "  (flows): ModuleList(\n",
            "    (0): AR_Step(\n",
            "      (conv): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
            "      (lstm): LSTM(1664, 1024, num_layers=2)\n",
            "      (attention_lstm): LSTM(80, 1024)\n",
            "      (attention_layer): Attention(\n",
            "        (softmax): Softmax(dim=2)\n",
            "        (query): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
            "        )\n",
            "        (key): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "        )\n",
            "        (value): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "        )\n",
            "        (v): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=640, out_features=1, bias=False)\n",
            "        )\n",
            "      )\n",
            "      (dense_layer): DenseLayer(\n",
            "        (layers): ModuleList(\n",
            "          (0): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "          (1): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (1): AR_Back_Step(\n",
            "      (ar_step): AR_Step(\n",
            "        (conv): Conv1d(1024, 160, kernel_size=(1,), stride=(1,))\n",
            "        (lstm): LSTM(1664, 1024, num_layers=2)\n",
            "        (attention_lstm): LSTM(80, 1024)\n",
            "        (attention_layer): Attention(\n",
            "          (softmax): Softmax(dim=2)\n",
            "          (query): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=1024, out_features=640, bias=False)\n",
            "          )\n",
            "          (key): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "          )\n",
            "          (value): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=640, out_features=640, bias=False)\n",
            "          )\n",
            "          (v): LinearNorm(\n",
            "            (linear_layer): Linear(in_features=640, out_features=1, bias=False)\n",
            "          )\n",
            "        )\n",
            "        (dense_layer): DenseLayer(\n",
            "          (layers): ModuleList(\n",
            "            (0): LinearNorm(\n",
            "              (linear_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "            (1): LinearNorm(\n",
            "              (linear_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (gate_layer): LinearNorm(\n",
            "          (linear_layer): Linear(in_features=1664, out_features=1, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (convolutions): ModuleList(\n",
            "      (0): Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      )\n",
            "      (1): Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      )\n",
            "      (2): Sequential(\n",
            "        (0): ConvNorm(\n",
            "          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "        )\n",
            "        (1): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "      )\n",
            "    )\n",
            "    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
            "  )\n",
            ")\n",
            "Number of speakers : 1\n",
            "output directory outdir\n",
            "Epoch: 0\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "1:\t-0.239364117\n",
            "2:\t-0.299374282\n",
            "3:\t-0.452334762\n",
            "4:\t-0.560164809\n",
            "5:\t-0.584067523\n",
            "6:\t-0.628925920\n",
            "7:\t-0.652656674\n",
            "8:\t-0.683006763\n",
            "9:\t-0.694110513\n",
            "10:\t-0.701363027\n",
            "11:\t-0.735207915\n",
            "12:\t-0.742987454\n",
            "13:\t-0.696931124\n",
            "14:\t-0.771421969\n",
            "15:\t-0.758963943\n",
            "16:\t-0.750936210\n",
            "17:\t-0.754111946\n",
            "18:\t-0.806477964\n",
            "19:\t-0.794726253\n",
            "20:\t-0.779358983\n",
            "21:\t-0.771025181\n",
            "22:\t-0.776612997\n",
            "23:\t-0.768508613\n",
            "24:\t-0.757471204\n",
            "25:\t-0.785332739\n",
            "26:\t-0.811225712\n",
            "27:\t-0.857131422\n",
            "28:\t-0.856264591\n",
            "29:\t-0.845274806\n",
            "30:\t-0.808268309\n",
            "31:\t-0.830900550\n",
            "32:\t-0.897026539\n",
            "33:\t-0.823318481\n",
            "34:\t-0.891926408\n",
            "35:\t-0.819160879\n",
            "36:\t-0.873903632\n",
            "37:\t-0.813843369\n",
            "38:\t-0.796089888\n",
            "39:\t-0.827078044\n",
            "40:\t-0.805310726\n",
            "41:\t-0.799032509\n",
            "42:\t-0.897672415\n",
            "43:\t-0.968260765\n",
            "44:\t-0.922996104\n",
            "45:\t-0.887871921\n",
            "46:\t-0.863609135\n",
            "47:\t-0.885666728\n",
            "48:\t-0.875194252\n",
            "49:\t-0.902832687\n",
            "50:\t-0.901352048\n",
            "51:\t-0.914708436\n",
            "52:\t-0.877492964\n",
            "53:\t-0.817245960\n",
            "54:\t-0.844196975\n",
            "55:\t-0.830870330\n",
            "56:\t-0.845787883\n",
            "57:\t-0.871087015\n",
            "58:\t-0.901949942\n",
            "59:\t-0.918777227\n",
            "60:\t-0.860099792\n",
            "61:\t-0.879370332\n",
            "62:\t-0.846054316\n",
            "63:\t-0.860183358\n",
            "64:\t-0.886968434\n",
            "65:\t-0.901722789\n",
            "66:\t-0.819148242\n",
            "67:\t-0.827769756\n",
            "68:\t-0.917995334\n",
            "69:\t-0.876463711\n",
            "70:\t-0.823487401\n",
            "71:\t-0.844808936\n",
            "72:\t-0.958761513\n",
            "73:\t-0.960601509\n",
            "74:\t-0.894156337\n",
            "75:\t-0.919731438\n",
            "76:\t-0.933121443\n",
            "77:\t-0.834029257\n",
            "78:\t-0.866372824\n",
            "79:\t-0.840925634\n",
            "80:\t-0.884086788\n",
            "81:\t-0.863102198\n",
            "82:\t-0.937940598\n",
            "83:\t-0.840085626\n",
            "84:\t-0.828143179\n",
            "85:\t-0.894701183\n",
            "86:\t-0.873173296\n",
            "87:\t-0.917271852\n",
            "88:\t-0.845037580\n",
            "89:\t-0.872551858\n",
            "90:\t-0.861848652\n",
            "91:\t-0.917359769\n",
            "92:\t-0.884011686\n",
            "93:\t-0.845394969\n",
            "94:\t-0.896119833\n",
            "95:\t-0.885274768\n",
            "96:\t-0.906813920\n",
            "97:\t-0.823393226\n",
            "98:\t-0.881593645\n",
            "99:\t-0.872422934\n",
            "100:\t-0.983149469\n",
            "101:\t-0.943589032\n",
            "102:\t-0.843961775\n",
            "103:\t-0.895737886\n",
            "104:\t-0.876533151\n",
            "105:\t-0.860462606\n",
            "106:\t-0.888829291\n",
            "107:\t-0.940198362\n",
            "108:\t-0.929963768\n",
            "109:\t-0.940743506\n",
            "110:\t-0.863978982\n",
            "111:\t-0.894207537\n",
            "112:\t-0.936556518\n",
            "113:\t-0.828728676\n",
            "114:\t-0.862182021\n",
            "115:\t-0.939305961\n",
            "116:\t-0.842115462\n",
            "117:\t-0.894189179\n",
            "118:\t-0.930170715\n",
            "119:\t-0.839270711\n",
            "120:\t-0.868687987\n",
            "121:\t-0.809440494\n",
            "122:\t-0.883180022\n",
            "123:\t-0.908698618\n",
            "124:\t-0.921452761\n",
            "125:\t-0.869320035\n",
            "126:\t-0.877947211\n",
            "127:\t-0.857718349\n",
            "128:\t-0.907523334\n",
            "129:\t-0.843921244\n",
            "130:\t-0.871347964\n",
            "131:\t-0.999131560\n",
            "132:\t-0.903582871\n",
            "133:\t-0.896682203\n",
            "134:\t-0.917871833\n",
            "135:\t-0.808705509\n",
            "136:\t-0.857158363\n",
            "137:\t-0.995769620\n",
            "138:\t-0.903121531\n",
            "139:\t-0.836250246\n",
            "140:\t-0.987423778\n",
            "141:\t-0.842923164\n",
            "142:\t-0.877335608\n",
            "143:\t-0.861718714\n",
            "144:\t-0.961000860\n",
            "145:\t-0.952256918\n",
            "146:\t-0.865986586\n",
            "147:\t-0.867083848\n",
            "148:\t-1.010894060\n",
            "149:\t-0.853152514\n",
            "150:\t-0.919325829\n",
            "151:\t-0.799738586\n",
            "152:\t-0.900173306\n",
            "153:\t-0.930283010\n",
            "154:\t-1.002619028\n",
            "155:\t-0.863795578\n",
            "156:\t-0.860343933\n",
            "157:\t-0.888925970\n",
            "158:\t-0.889784038\n",
            "159:\t-0.867333233\n",
            "160:\t-0.869930267\n",
            "161:\t-1.024742961\n",
            "162:\t-0.868388414\n",
            "163:\t-0.908901274\n",
            "164:\t-0.901821315\n",
            "165:\t-0.891395450\n",
            "166:\t-0.862839222\n",
            "167:\t-0.902104080\n",
            "168:\t-0.914453983\n",
            "169:\t-0.889777124\n",
            "170:\t-0.915309370\n",
            "171:\t-0.963149071\n",
            "172:\t-0.786763370\n",
            "173:\t-0.843428612\n",
            "174:\t-0.928786337\n",
            "175:\t-0.856121123\n",
            "176:\t-0.909363210\n",
            "177:\t-0.908153832\n",
            "178:\t-0.843281806\n",
            "179:\t-0.902940154\n",
            "180:\t-0.957591891\n",
            "181:\t-0.945380211\n",
            "182:\t-0.870286524\n",
            "183:\t-0.918674409\n",
            "184:\t-0.913760781\n",
            "185:\t-0.933108330\n",
            "186:\t-1.003440142\n",
            "187:\t-0.958020627\n",
            "188:\t-0.968307137\n",
            "189:\t-0.901928067\n",
            "190:\t-0.912101269\n",
            "191:\t-0.858968198\n",
            "192:\t-0.858347297\n",
            "193:\t-0.885364354\n",
            "194:\t-0.834137142\n",
            "195:\t-0.861927688\n",
            "196:\t-0.846755266\n",
            "197:\t-0.874803722\n",
            "198:\t-0.874600589\n",
            "199:\t-0.873786628\n",
            "200:\t-0.978066564\n",
            "201:\t-0.865692854\n",
            "202:\t-0.877522469\n",
            "203:\t-0.836328983\n",
            "204:\t-0.902179599\n",
            "205:\t-0.878743231\n",
            "206:\t-0.858170509\n",
            "207:\t-0.890868902\n",
            "208:\t-0.894611120\n",
            "209:\t-0.930859625\n",
            "210:\t-0.906578362\n",
            "211:\t-0.935944974\n",
            "212:\t-0.873375893\n",
            "213:\t-0.878052413\n",
            "214:\t-0.972557425\n",
            "215:\t-0.862103701\n",
            "216:\t-1.022852182\n",
            "217:\t-0.885788620\n",
            "218:\t-0.884357810\n",
            "219:\t-0.885859609\n",
            "220:\t-0.806603551\n",
            "221:\t-0.875043154\n",
            "222:\t-0.854518592\n",
            "223:\t-0.890374005\n",
            "224:\t-0.945955336\n",
            "225:\t-0.870295227\n",
            "226:\t-0.860117078\n",
            "227:\t-0.882442772\n",
            "228:\t-0.883523345\n",
            "229:\t-0.854703784\n",
            "230:\t-0.892270267\n",
            "231:\t-0.917615235\n",
            "232:\t-0.903332949\n",
            "233:\t-0.869390190\n",
            "234:\t-0.880278766\n",
            "235:\t-0.859016597\n",
            "236:\t-0.930723965\n",
            "237:\t-0.891483307\n",
            "238:\t-0.834636688\n",
            "239:\t-0.911159337\n",
            "240:\t-0.887290597\n",
            "241:\t-0.878350377\n",
            "242:\t-0.881318629\n",
            "243:\t-0.869248390\n",
            "244:\t-0.938619018\n",
            "245:\t-0.811552525\n",
            "246:\t-0.883744121\n",
            "247:\t-0.853462994\n",
            "248:\t-0.928210378\n",
            "249:\t-0.948862076\n",
            "250:\t-0.922889650\n",
            "251:\t-0.845905066\n",
            "252:\t-0.873381972\n",
            "253:\t-0.973573446\n",
            "254:\t-0.865971386\n",
            "255:\t-0.923980594\n",
            "256:\t-0.955470800\n",
            "257:\t-1.031262517\n",
            "258:\t-1.001180053\n",
            "259:\t-0.856939912\n",
            "260:\t-0.870153785\n",
            "261:\t-0.986028254\n",
            "262:\t-0.866390407\n",
            "263:\t-0.871304989\n",
            "264:\t-0.858196378\n",
            "265:\t-0.917853117\n",
            "266:\t-0.831043720\n",
            "267:\t-0.953556895\n",
            "268:\t-0.969298482\n",
            "269:\t-0.918107629\n",
            "270:\t-0.873692989\n",
            "271:\t-1.007555604\n",
            "272:\t-0.966232717\n",
            "273:\t-0.935716391\n",
            "274:\t-0.833327770\n",
            "275:\t-0.953640401\n",
            "276:\t-0.950056970\n",
            "277:\t-0.959285021\n",
            "278:\t-0.909697711\n",
            "279:\t-0.909840167\n",
            "280:\t-0.854755819\n",
            "281:\t-0.918451607\n",
            "282:\t-0.975771666\n",
            "283:\t-0.890857160\n",
            "284:\t-0.851492822\n",
            "285:\t-0.871687531\n",
            "286:\t-0.851244688\n",
            "287:\t-0.883527637\n",
            "288:\t-0.971561015\n",
            "289:\t-0.958233058\n",
            "290:\t-0.916690409\n",
            "291:\t-0.911726594\n",
            "292:\t-0.898527801\n",
            "293:\t-0.884197772\n",
            "294:\t-0.892493904\n",
            "295:\t-0.893758655\n",
            "296:\t-0.963726819\n",
            "297:\t-0.862335265\n",
            "298:\t-0.912053227\n",
            "299:\t-0.985721111\n",
            "300:\t-0.854968488\n",
            "301:\t-0.896360457\n",
            "302:\t-0.850771368\n",
            "303:\t-0.914070427\n",
            "304:\t-0.925538361\n",
            "305:\t-1.013826728\n",
            "306:\t-0.957238078\n",
            "307:\t-0.875852823\n",
            "308:\t-0.932911932\n",
            "309:\t-0.898115933\n",
            "310:\t-1.055917621\n",
            "311:\t-0.922967970\n",
            "312:\t-0.995684087\n",
            "313:\t-0.903797984\n",
            "314:\t-0.899892569\n",
            "315:\t-0.975672066\n",
            "316:\t-0.900981963\n",
            "317:\t-0.967446744\n",
            "318:\t-0.922063708\n",
            "319:\t-0.928327262\n",
            "320:\t-1.017159820\n",
            "321:\t-0.874933004\n",
            "322:\t-0.874411345\n",
            "323:\t-0.864791214\n",
            "324:\t-0.945982933\n",
            "325:\t-0.876772761\n",
            "326:\t-1.009238005\n",
            "327:\t-0.869778097\n",
            "328:\t-0.883924246\n",
            "329:\t-1.001510978\n",
            "330:\t-0.877899051\n",
            "331:\t-1.022270799\n",
            "332:\t-0.899180532\n",
            "333:\t-0.970077515\n",
            "334:\t-0.883674979\n",
            "335:\t-0.933507264\n",
            "336:\t-0.979620576\n",
            "337:\t-0.930168152\n",
            "338:\t-1.065982819\n",
            "339:\t-0.883398652\n",
            "340:\t-0.881596684\n",
            "341:\t-0.838173807\n",
            "342:\t-0.924284577\n",
            "343:\t-0.863003492\n",
            "344:\t-0.951836109\n",
            "345:\t-0.977564633\n",
            "346:\t-0.888815582\n",
            "347:\t-1.032421589\n",
            "348:\t-0.885214031\n",
            "349:\t-0.982990503\n",
            "350:\t-0.868189156\n",
            "351:\t-0.822874248\n",
            "352:\t-0.956547260\n",
            "353:\t-0.862796783\n",
            "354:\t-0.930227578\n",
            "355:\t-0.897587895\n",
            "356:\t-1.038701415\n",
            "357:\t-0.875722766\n",
            "358:\t-0.916949332\n",
            "359:\t-1.019786358\n",
            "360:\t-0.878116667\n",
            "361:\t-0.913917124\n",
            "362:\t-0.844340801\n",
            "363:\t-0.824320257\n",
            "364:\t-0.877479792\n",
            "365:\t-0.904385507\n",
            "366:\t-0.979061663\n",
            "367:\t-0.933968782\n",
            "368:\t-0.982676327\n",
            "369:\t-0.869945049\n",
            "370:\t-0.866917253\n",
            "371:\t-0.946716785\n",
            "372:\t-0.961105943\n",
            "373:\t-0.891429007\n",
            "374:\t-0.869751334\n",
            "375:\t-0.888561010\n",
            "376:\t-1.006349802\n",
            "377:\t-0.948785782\n",
            "378:\t-0.987639904\n",
            "379:\t-0.902006805\n",
            "380:\t-0.875604212\n",
            "381:\t-0.985967994\n",
            "382:\t-0.975750923\n",
            "383:\t-1.020598888\n",
            "384:\t-1.010683775\n",
            "385:\t-0.852856636\n",
            "386:\t-0.886799514\n",
            "387:\t-0.882422090\n",
            "388:\t-0.863987207\n",
            "389:\t-0.884597778\n",
            "390:\t-0.851905763\n",
            "391:\t-0.962126732\n",
            "392:\t-0.893265247\n",
            "393:\t-0.990074098\n",
            "394:\t-0.866905630\n",
            "395:\t-1.049552202\n",
            "396:\t-0.915529311\n",
            "397:\t-1.011094928\n",
            "398:\t-0.928713143\n",
            "399:\t-0.897505164\n",
            "400:\t-0.872015119\n",
            "401:\t-0.886284709\n",
            "402:\t-0.985003173\n",
            "403:\t-0.845226049\n",
            "404:\t-0.958786488\n",
            "405:\t-0.945707262\n",
            "406:\t-0.876195490\n",
            "407:\t-0.861669481\n",
            "408:\t-0.883580744\n",
            "409:\t-0.887674212\n",
            "410:\t-1.005179167\n",
            "411:\t-0.889219105\n",
            "412:\t-1.022178888\n",
            "413:\t-0.937989116\n",
            "414:\t-1.021164417\n",
            "415:\t-0.863497257\n",
            "416:\t-0.872828066\n",
            "417:\t-0.845434427\n",
            "418:\t-0.954695880\n",
            "419:\t-1.032100081\n",
            "420:\t-0.849034369\n",
            "421:\t-0.850844085\n",
            "422:\t-0.895189285\n",
            "423:\t-0.948678851\n",
            "424:\t-0.893403411\n",
            "425:\t-1.021301866\n",
            "426:\t-0.897766888\n",
            "427:\t-0.789988279\n",
            "428:\t-0.833252251\n",
            "429:\t-0.902768314\n",
            "430:\t-0.945194602\n",
            "431:\t-0.902269304\n",
            "432:\t-0.849783361\n",
            "433:\t-0.871617079\n",
            "434:\t-0.919404387\n",
            "435:\t-0.850097895\n",
            "436:\t-0.890897155\n",
            "437:\t-0.942971587\n",
            "438:\t-0.998627007\n",
            "439:\t-0.973414838\n",
            "440:\t-0.888163924\n",
            "441:\t-1.015308022\n",
            "442:\t-1.002664566\n",
            "443:\t-0.987438202\n",
            "444:\t-0.971781194\n",
            "445:\t-0.973237753\n",
            "446:\t-0.893399656\n",
            "447:\t-0.944106102\n",
            "448:\t-0.883140683\n",
            "449:\t-0.873470664\n",
            "450:\t-0.930780411\n",
            "451:\t-0.908047855\n",
            "452:\t-0.845935404\n",
            "453:\t-1.033096552\n",
            "454:\t-0.872281551\n",
            "455:\t-1.024145126\n",
            "456:\t-0.879700601\n",
            "457:\t-0.862863600\n",
            "458:\t-0.995973229\n",
            "459:\t-0.772816718\n",
            "460:\t-0.857399106\n",
            "461:\t-0.941489160\n",
            "Epoch: 1\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "462:\t-0.981003702\n",
            "463:\t-0.942574203\n",
            "464:\t-0.867243648\n",
            "465:\t-0.977520287\n",
            "466:\t-0.915979087\n",
            "467:\t-0.877627313\n",
            "468:\t-0.987071574\n",
            "469:\t-0.887014449\n",
            "470:\t-0.878845632\n",
            "471:\t-0.877622485\n",
            "472:\t-0.856295466\n",
            "473:\t-0.868179440\n",
            "474:\t-0.907509565\n",
            "475:\t-0.880218267\n",
            "476:\t-0.919838071\n",
            "477:\t-0.881772816\n",
            "478:\t-0.904263556\n",
            "479:\t-1.041258335\n",
            "480:\t-0.996316016\n",
            "481:\t-0.959787548\n",
            "482:\t-0.947313666\n",
            "483:\t-0.867616177\n",
            "484:\t-0.876556456\n",
            "485:\t-1.033886194\n",
            "486:\t-0.887520909\n",
            "487:\t-0.979344785\n",
            "488:\t-0.873336017\n",
            "489:\t-1.055230379\n",
            "490:\t-1.003292203\n",
            "491:\t-1.015755296\n",
            "492:\t-0.901165307\n",
            "493:\t-1.137452364\n",
            "494:\t-0.887764335\n",
            "495:\t-0.896465719\n",
            "496:\t-0.939970911\n",
            "497:\t-0.873846352\n",
            "498:\t-0.903360128\n",
            "499:\t-0.902253509\n",
            "500:\t-0.922940671\n",
            "Saving model and optimizer state at iteration 500 to outdir/model_500\n",
            "501:\t-0.995254934\n",
            "502:\t-1.008164525\n",
            "503:\t-0.895252705\n",
            "504:\t-0.930113256\n",
            "505:\t-0.960093856\n",
            "506:\t-0.999240756\n",
            "507:\t-0.918434143\n",
            "508:\t-1.015729308\n",
            "509:\t-1.016180754\n",
            "510:\t-0.884308457\n",
            "511:\t-0.878438532\n",
            "512:\t-0.941021502\n",
            "513:\t-0.894078016\n",
            "514:\t-0.919073045\n",
            "515:\t-0.969471812\n",
            "516:\t-0.881878495\n",
            "517:\t-0.938198507\n",
            "518:\t-0.902254462\n",
            "519:\t-0.927029610\n",
            "520:\t-0.901103914\n",
            "521:\t-0.878535032\n",
            "522:\t-0.880984902\n",
            "523:\t-0.905492485\n",
            "524:\t-0.953679919\n",
            "525:\t-0.932975769\n",
            "526:\t-1.037245393\n",
            "527:\t-1.040023685\n",
            "528:\t-0.853139758\n",
            "529:\t-0.907345355\n",
            "530:\t-0.881480515\n",
            "531:\t-1.046669006\n",
            "532:\t-0.870195091\n",
            "533:\t-0.978189945\n",
            "534:\t-1.050221086\n",
            "535:\t-0.870779753\n",
            "536:\t-0.985999942\n",
            "537:\t-1.077318668\n",
            "538:\t-0.979469836\n",
            "539:\t-0.927804053\n",
            "540:\t-0.874213159\n",
            "541:\t-0.955792248\n",
            "542:\t-0.921297431\n",
            "543:\t-0.884779334\n",
            "544:\t-1.002783298\n",
            "545:\t-0.950092912\n",
            "546:\t-0.969050825\n",
            "547:\t-0.966897488\n",
            "548:\t-1.085292220\n",
            "549:\t-0.879839718\n",
            "550:\t-0.886992872\n",
            "551:\t-0.865051329\n",
            "552:\t-0.939583778\n",
            "553:\t-0.912559390\n",
            "554:\t-0.929600120\n",
            "555:\t-0.914450526\n",
            "556:\t-0.945758402\n",
            "557:\t-0.895437181\n",
            "558:\t-0.964867473\n",
            "559:\t-0.972476304\n",
            "560:\t-0.932452023\n",
            "561:\t-0.843670249\n",
            "562:\t-0.915719271\n",
            "563:\t-0.973640442\n",
            "564:\t-0.990806818\n",
            "565:\t-0.903884590\n",
            "566:\t-0.920771956\n",
            "567:\t-1.001296401\n",
            "568:\t-0.953153670\n",
            "569:\t-0.991447091\n",
            "570:\t-0.992071152\n",
            "571:\t-0.983709812\n",
            "572:\t-0.965798616\n",
            "573:\t-0.885595858\n",
            "574:\t-0.871502757\n",
            "575:\t-1.041842580\n",
            "576:\t-0.886725545\n",
            "577:\t-0.951427341\n",
            "578:\t-0.977966607\n",
            "579:\t-0.928034306\n",
            "580:\t-0.999825478\n",
            "581:\t-0.893999994\n",
            "582:\t-0.926711380\n",
            "583:\t-0.962146461\n",
            "584:\t-0.875024199\n",
            "585:\t-0.893028438\n",
            "586:\t-0.958295107\n",
            "587:\t-1.053159118\n",
            "588:\t-0.890294731\n",
            "589:\t-0.915707767\n",
            "590:\t-0.918545306\n",
            "591:\t-1.008792043\n",
            "592:\t-0.884162247\n",
            "593:\t-1.028251648\n",
            "594:\t-0.885380268\n",
            "595:\t-1.061473608\n",
            "596:\t-0.893223464\n",
            "597:\t-0.973794103\n",
            "598:\t-0.881802082\n",
            "599:\t-0.897927821\n",
            "600:\t-0.879595518\n",
            "601:\t-1.047500968\n",
            "602:\t-1.035196781\n",
            "603:\t-0.933935285\n",
            "604:\t-0.993221402\n",
            "605:\t-0.844952941\n",
            "606:\t-1.029600263\n",
            "607:\t-0.906997204\n",
            "608:\t-1.045401931\n",
            "609:\t-0.888014376\n",
            "610:\t-0.906413317\n",
            "611:\t-0.869372010\n",
            "612:\t-0.858278036\n",
            "613:\t-0.885653019\n",
            "614:\t-0.897850454\n",
            "615:\t-0.903497398\n",
            "616:\t-0.939866841\n",
            "617:\t-0.910819650\n",
            "618:\t-0.909521341\n",
            "619:\t-0.980297506\n",
            "620:\t-0.896268368\n",
            "621:\t-0.885348141\n",
            "622:\t-1.013651967\n",
            "623:\t-0.863294482\n",
            "624:\t-0.955240726\n",
            "625:\t-0.992827058\n",
            "626:\t-0.924486876\n",
            "627:\t-0.909133375\n",
            "628:\t-1.013388872\n",
            "629:\t-0.930546463\n",
            "630:\t-0.962614834\n",
            "631:\t-0.862012208\n",
            "632:\t-0.896129370\n",
            "633:\t-0.965221941\n",
            "634:\t-0.874636471\n",
            "635:\t-1.040239215\n",
            "636:\t-0.968425333\n",
            "637:\t-1.095875263\n",
            "638:\t-0.886514664\n",
            "639:\t-0.912593901\n",
            "640:\t-1.068424344\n",
            "641:\t-0.908300161\n",
            "642:\t-1.049704671\n",
            "643:\t-0.846477687\n",
            "644:\t-0.837924480\n",
            "645:\t-0.997411013\n",
            "646:\t-1.056336880\n",
            "647:\t-0.916337252\n",
            "648:\t-0.972317278\n",
            "649:\t-0.910152137\n",
            "650:\t-0.877707243\n",
            "651:\t-0.978199005\n",
            "652:\t-0.883856773\n",
            "653:\t-1.046181321\n",
            "654:\t-0.963393271\n",
            "655:\t-0.852426589\n",
            "656:\t-1.101051092\n",
            "657:\t-1.046116352\n",
            "658:\t-0.931456983\n",
            "659:\t-0.993161738\n",
            "660:\t-1.007330418\n",
            "661:\t-0.863881230\n",
            "662:\t-0.899871707\n",
            "663:\t-0.772307158\n",
            "664:\t-0.970509410\n",
            "665:\t-0.947439253\n",
            "666:\t-0.881318450\n",
            "667:\t-0.981456161\n",
            "668:\t-0.850410521\n",
            "669:\t-0.942586184\n",
            "670:\t-0.891033173\n",
            "671:\t-0.943185985\n",
            "672:\t-0.945741057\n",
            "673:\t-0.894730508\n",
            "674:\t-0.873632669\n",
            "675:\t-0.959011555\n",
            "676:\t-0.959007800\n",
            "677:\t-0.937501431\n",
            "678:\t-0.882198930\n",
            "679:\t-0.871985972\n",
            "680:\t-0.899368703\n",
            "681:\t-1.020588994\n",
            "682:\t-0.885627031\n",
            "683:\t-0.971400201\n",
            "684:\t-0.946858048\n",
            "685:\t-0.995148242\n",
            "686:\t-0.929178655\n",
            "687:\t-0.893788159\n",
            "688:\t-0.956182420\n",
            "689:\t-0.920893371\n",
            "690:\t-0.956031680\n",
            "691:\t-0.958454549\n",
            "692:\t-0.847375512\n",
            "693:\t-0.958906770\n",
            "694:\t-0.894392610\n",
            "695:\t-0.903101146\n",
            "696:\t-0.915168166\n",
            "697:\t-0.986789227\n",
            "698:\t-0.965222597\n",
            "699:\t-0.953645468\n",
            "700:\t-0.873948932\n",
            "701:\t-1.030436039\n",
            "702:\t-0.977442622\n",
            "703:\t-1.000826836\n",
            "704:\t-1.029921770\n",
            "705:\t-0.913335979\n",
            "706:\t-0.919910789\n",
            "707:\t-1.012767673\n",
            "708:\t-0.907352686\n",
            "709:\t-0.881020188\n",
            "710:\t-0.887030840\n",
            "711:\t-0.949588358\n",
            "712:\t-0.950855851\n",
            "713:\t-0.925232112\n",
            "714:\t-0.911866128\n",
            "715:\t-1.015627503\n",
            "716:\t-1.067651153\n",
            "717:\t-0.890623569\n",
            "718:\t-0.912792683\n",
            "719:\t-0.930166900\n",
            "720:\t-0.910254955\n",
            "721:\t-0.932321489\n",
            "722:\t-1.027357936\n",
            "723:\t-1.003820419\n",
            "724:\t-0.920340717\n",
            "725:\t-0.971991897\n",
            "726:\t-0.927538931\n",
            "727:\t-0.879791677\n",
            "728:\t-0.904928267\n",
            "729:\t-0.961379349\n",
            "730:\t-0.919672847\n",
            "731:\t-0.908402383\n",
            "732:\t-0.908281267\n",
            "733:\t-0.953586876\n",
            "734:\t-0.903068841\n",
            "735:\t-1.037783980\n",
            "736:\t-0.911648631\n",
            "737:\t-0.890950739\n",
            "738:\t-1.015700936\n",
            "739:\t-0.894970894\n",
            "740:\t-1.006083727\n",
            "741:\t-0.981126249\n",
            "742:\t-0.873999953\n",
            "743:\t-0.981658578\n",
            "744:\t-0.957612216\n",
            "745:\t-0.901533306\n",
            "746:\t-1.023819804\n",
            "747:\t-0.886903763\n",
            "748:\t-1.018133044\n",
            "749:\t-0.884218574\n",
            "750:\t-0.925975263\n",
            "751:\t-0.926755726\n",
            "752:\t-0.903150380\n",
            "753:\t-1.103798151\n",
            "754:\t-0.822715521\n",
            "755:\t-0.919772506\n",
            "756:\t-0.941065609\n",
            "757:\t-0.899590015\n",
            "758:\t-0.886644006\n",
            "759:\t-0.910736442\n",
            "760:\t-0.903243423\n",
            "761:\t-0.908342600\n",
            "762:\t-0.927507758\n",
            "763:\t-1.031192541\n",
            "764:\t-1.026516199\n",
            "765:\t-1.029085398\n",
            "766:\t-0.905777752\n",
            "767:\t-1.059924722\n",
            "768:\t-0.913058758\n",
            "769:\t-0.883591413\n",
            "770:\t-0.929485142\n",
            "771:\t-0.900988519\n",
            "772:\t-0.980331361\n",
            "773:\t-0.923615694\n",
            "774:\t-0.887039244\n",
            "775:\t-0.965004027\n",
            "776:\t-0.906815886\n",
            "777:\t-0.906365633\n",
            "778:\t-1.043429255\n",
            "779:\t-1.021299839\n",
            "780:\t-0.885373533\n",
            "781:\t-0.964725733\n",
            "782:\t-0.983538330\n",
            "783:\t-1.031621099\n",
            "784:\t-0.988267303\n",
            "785:\t-0.876238585\n",
            "786:\t-0.894989073\n",
            "787:\t-0.921590745\n",
            "788:\t-0.906624138\n",
            "789:\t-0.908032954\n",
            "790:\t-1.063032269\n",
            "791:\t-1.046419859\n",
            "792:\t-0.943696439\n",
            "793:\t-0.881050587\n",
            "794:\t-1.094972610\n",
            "795:\t-0.888963759\n",
            "796:\t-0.894688308\n",
            "797:\t-0.953130603\n",
            "798:\t-0.893256426\n",
            "799:\t-1.076078534\n",
            "800:\t-0.989178300\n",
            "801:\t-0.898103058\n",
            "802:\t-1.002077103\n",
            "803:\t-1.072753906\n",
            "804:\t-0.994586945\n",
            "805:\t-0.910468161\n",
            "806:\t-1.027668595\n",
            "807:\t-0.884081960\n",
            "808:\t-0.862953007\n",
            "809:\t-1.041571259\n",
            "810:\t-0.968823791\n",
            "811:\t-1.055739284\n",
            "812:\t-0.882302642\n",
            "813:\t-0.987417281\n",
            "814:\t-1.003950715\n",
            "815:\t-0.959685922\n",
            "816:\t-0.900431991\n",
            "817:\t-0.952798605\n",
            "818:\t-1.110908628\n",
            "819:\t-0.896107018\n",
            "820:\t-1.053167820\n",
            "821:\t-1.069003463\n",
            "822:\t-0.898872912\n",
            "823:\t-0.997028649\n",
            "824:\t-0.941629529\n",
            "825:\t-0.883293569\n",
            "826:\t-0.886241496\n",
            "827:\t-0.886217356\n",
            "828:\t-1.055286169\n",
            "829:\t-1.043369770\n",
            "830:\t-0.904852748\n",
            "831:\t-1.080228329\n",
            "832:\t-0.878410339\n",
            "833:\t-0.947853088\n",
            "834:\t-0.914159656\n",
            "835:\t-0.867273331\n",
            "836:\t-0.899043322\n",
            "837:\t-0.888861120\n",
            "838:\t-0.872171104\n",
            "839:\t-0.973987520\n",
            "840:\t-1.008220077\n",
            "841:\t-1.064736843\n",
            "842:\t-0.871955574\n",
            "843:\t-0.906666398\n",
            "844:\t-0.872573197\n",
            "845:\t-0.939209223\n",
            "846:\t-0.935832441\n",
            "847:\t-0.999761879\n",
            "848:\t-0.996132672\n",
            "849:\t-0.985198736\n",
            "850:\t-0.862368226\n",
            "851:\t-0.912902474\n",
            "852:\t-0.990365088\n",
            "853:\t-0.979259551\n",
            "854:\t-0.993112862\n",
            "855:\t-0.890136778\n",
            "856:\t-0.935061753\n",
            "857:\t-0.943596542\n",
            "858:\t-0.979513705\n",
            "859:\t-0.954706192\n",
            "860:\t-0.894483387\n",
            "861:\t-0.867141485\n",
            "862:\t-0.946143329\n",
            "863:\t-0.998882353\n",
            "864:\t-1.051700473\n",
            "865:\t-1.099609971\n",
            "866:\t-1.070504785\n",
            "867:\t-0.991704464\n",
            "868:\t-0.906301439\n",
            "869:\t-1.078549504\n",
            "870:\t-0.930678904\n",
            "871:\t-0.922520280\n",
            "872:\t-0.873442054\n",
            "873:\t-0.889189482\n",
            "874:\t-0.917034566\n",
            "875:\t-0.891068161\n",
            "876:\t-0.890715241\n",
            "877:\t-0.941998005\n",
            "878:\t-0.863424182\n",
            "879:\t-0.893147409\n",
            "880:\t-1.080134153\n",
            "881:\t-0.902662992\n",
            "882:\t-0.903530538\n",
            "883:\t-1.019994140\n",
            "884:\t-1.031444311\n",
            "885:\t-1.088061094\n",
            "886:\t-0.905296564\n",
            "887:\t-1.014534950\n",
            "888:\t-0.881641924\n",
            "889:\t-0.885748923\n",
            "890:\t-1.074415803\n",
            "891:\t-1.012068152\n",
            "892:\t-1.105076194\n",
            "893:\t-0.814475119\n",
            "894:\t-0.927962422\n",
            "895:\t-0.954425931\n",
            "896:\t-0.903636754\n",
            "897:\t-0.859873831\n",
            "898:\t-0.833063424\n",
            "899:\t-0.871552527\n",
            "900:\t-0.877274871\n",
            "901:\t-0.994969487\n",
            "902:\t-0.931915879\n",
            "903:\t-0.934857428\n",
            "904:\t-0.980335772\n",
            "905:\t-1.025370598\n",
            "906:\t-0.914484322\n",
            "907:\t-0.868464530\n",
            "908:\t-0.900854766\n",
            "909:\t-1.038348079\n",
            "910:\t-0.901460469\n",
            "911:\t-0.906610250\n",
            "912:\t-1.011663198\n",
            "913:\t-0.937611401\n",
            "914:\t-0.961067736\n",
            "915:\t-0.894215822\n",
            "916:\t-1.047649264\n",
            "917:\t-1.006531000\n",
            "918:\t-0.950213969\n",
            "919:\t-0.991916776\n",
            "920:\t-0.867236257\n",
            "921:\t-0.912697494\n",
            "922:\t-1.055361986\n",
            "Epoch: 2\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "923:\t-0.931394398\n",
            "924:\t-1.019949198\n",
            "925:\t-0.975841999\n",
            "926:\t-1.071954608\n",
            "927:\t-0.879890800\n",
            "928:\t-0.877352834\n",
            "929:\t-1.090435028\n",
            "930:\t-1.027634382\n",
            "931:\t-0.915139377\n",
            "932:\t-0.970813572\n",
            "933:\t-0.877083480\n",
            "934:\t-0.899992526\n",
            "935:\t-0.912090003\n",
            "936:\t-0.976344705\n",
            "937:\t-0.886160076\n",
            "938:\t-0.866672397\n",
            "939:\t-0.879105985\n",
            "940:\t-0.904086828\n",
            "941:\t-0.915828645\n",
            "942:\t-0.871972799\n",
            "943:\t-0.887928009\n",
            "944:\t-0.939636767\n",
            "945:\t-0.937199473\n",
            "946:\t-0.923784912\n",
            "947:\t-0.993690073\n",
            "948:\t-0.907626629\n",
            "949:\t-0.947308958\n",
            "950:\t-0.932718158\n",
            "951:\t-0.948044538\n",
            "952:\t-0.925620377\n",
            "953:\t-1.047027469\n",
            "954:\t-0.920925379\n",
            "955:\t-0.881875038\n",
            "956:\t-0.895793974\n",
            "957:\t-0.913581491\n",
            "958:\t-1.092612624\n",
            "959:\t-0.926142752\n",
            "960:\t-0.901075482\n",
            "961:\t-1.078144670\n",
            "962:\t-0.884165347\n",
            "963:\t-1.086199999\n",
            "964:\t-1.032123446\n",
            "965:\t-1.079170108\n",
            "966:\t-0.909163237\n",
            "967:\t-0.985228598\n",
            "968:\t-0.986198664\n",
            "969:\t-0.850306988\n",
            "970:\t-0.917547703\n",
            "971:\t-0.977644503\n",
            "972:\t-0.935835063\n",
            "973:\t-0.909721613\n",
            "974:\t-0.875475943\n",
            "975:\t-1.065544724\n",
            "976:\t-0.901805639\n",
            "977:\t-0.988099515\n",
            "978:\t-1.033448935\n",
            "979:\t-0.933923066\n",
            "980:\t-0.986896396\n",
            "981:\t-0.877773643\n",
            "982:\t-0.943206072\n",
            "983:\t-1.018220544\n",
            "984:\t-0.896735072\n",
            "985:\t-0.870369852\n",
            "986:\t-0.919197202\n",
            "987:\t-0.977333486\n",
            "988:\t-0.982023716\n",
            "989:\t-0.987456024\n",
            "990:\t-0.892927706\n",
            "991:\t-0.928529799\n",
            "992:\t-0.976194143\n",
            "993:\t-0.939491570\n",
            "994:\t-1.104089975\n",
            "995:\t-0.962328792\n",
            "996:\t-0.895450294\n",
            "997:\t-0.929352641\n",
            "998:\t-0.939818323\n",
            "999:\t-1.010949016\n",
            "1000:\t-0.945327103\n",
            "Saving model and optimizer state at iteration 1000 to outdir/model_1000\n",
            "1001:\t-0.931884408\n",
            "1002:\t-1.029851913\n",
            "1003:\t-0.974336386\n",
            "1004:\t-0.883523643\n",
            "1005:\t-0.940281630\n",
            "1006:\t-0.940662980\n",
            "1007:\t-0.687612534\n",
            "1008:\t-1.047347307\n",
            "1009:\t-0.865127742\n",
            "1010:\t-0.961070061\n",
            "1011:\t-0.868068397\n",
            "1012:\t-0.969866395\n",
            "1013:\t-0.892149746\n",
            "1014:\t-0.863655269\n",
            "1015:\t-0.906936824\n",
            "1016:\t-0.936409235\n",
            "1017:\t-1.005813479\n",
            "1018:\t-0.912712395\n",
            "1019:\t-0.945877135\n",
            "1020:\t-1.038841844\n",
            "1021:\t-0.990968943\n",
            "1022:\t-0.973976135\n",
            "1023:\t-0.897841275\n",
            "1024:\t-0.898078084\n",
            "1025:\t-0.878657997\n",
            "1026:\t-1.003411055\n",
            "1027:\t-1.077155352\n",
            "1028:\t-0.964885354\n",
            "1029:\t-0.976939678\n",
            "1030:\t-0.953078747\n",
            "1031:\t-0.911826134\n",
            "1032:\t-0.885682344\n",
            "1033:\t-0.904891312\n",
            "1034:\t-1.061750174\n",
            "1035:\t-1.126746297\n",
            "1036:\t-0.909398139\n",
            "1037:\t-0.877679050\n",
            "1038:\t-1.018499970\n",
            "1039:\t-0.973007500\n",
            "1040:\t-1.044476151\n",
            "1041:\t-0.891308367\n",
            "1042:\t-0.994019210\n",
            "1043:\t-0.991039395\n",
            "1044:\t-1.018178701\n",
            "1045:\t-1.017727256\n",
            "1046:\t-0.871447802\n",
            "1047:\t-1.000758529\n",
            "1048:\t-1.063726187\n",
            "1049:\t-0.889903784\n",
            "1050:\t-0.946116328\n",
            "1051:\t-0.884667933\n",
            "1052:\t-0.906046987\n",
            "1053:\t-0.904932499\n",
            "1054:\t-0.932624519\n",
            "1055:\t-0.861498535\n",
            "1056:\t-0.925640881\n",
            "1057:\t-1.059036851\n",
            "1058:\t-0.883102357\n",
            "1059:\t-0.923608303\n",
            "1060:\t-0.902116776\n",
            "1061:\t-0.935218930\n",
            "1062:\t-0.986374855\n",
            "1063:\t-1.075555563\n",
            "1064:\t-0.934343338\n",
            "1065:\t-0.916223764\n",
            "1066:\t-0.911916912\n",
            "1067:\t-0.939697146\n",
            "1068:\t-0.949754834\n",
            "1069:\t-0.889101505\n",
            "1070:\t-0.902434111\n",
            "1071:\t-0.962829232\n",
            "1072:\t-1.063314795\n",
            "1073:\t-0.942484856\n",
            "1074:\t-0.984395802\n",
            "1075:\t-0.929342270\n",
            "1076:\t-0.958608985\n",
            "1077:\t-0.950977147\n",
            "1078:\t-0.885516405\n",
            "1079:\t-0.896126568\n",
            "1080:\t-1.011154890\n",
            "1081:\t-0.897630215\n",
            "1082:\t-0.971178114\n",
            "1083:\t-0.903338671\n",
            "1084:\t-0.891176164\n",
            "1085:\t-0.890014529\n",
            "1086:\t-0.869663239\n",
            "1087:\t-0.982400954\n",
            "1088:\t-1.034342527\n",
            "1089:\t-0.995718896\n",
            "1090:\t-0.913586736\n",
            "1091:\t-1.038607836\n",
            "1092:\t-1.072935939\n",
            "1093:\t-0.926318109\n",
            "1094:\t-0.894303143\n",
            "1095:\t-0.876042664\n",
            "1096:\t-0.889446080\n",
            "1097:\t-0.911664248\n",
            "1098:\t-1.028425336\n",
            "1099:\t-0.931234956\n",
            "1100:\t-0.964549899\n",
            "1101:\t-0.923855305\n",
            "1102:\t-1.028625607\n",
            "1103:\t-0.952557206\n",
            "1104:\t-0.925299525\n",
            "1105:\t-0.920343518\n",
            "1106:\t-1.060109735\n",
            "1107:\t-0.885569692\n",
            "1108:\t-0.911087215\n",
            "1109:\t-0.904513955\n",
            "1110:\t-0.965757370\n",
            "1111:\t-1.066793919\n",
            "1112:\t-0.885986984\n",
            "1113:\t-1.097726583\n",
            "1114:\t-1.056507707\n",
            "1115:\t-1.008086205\n",
            "1116:\t-1.078619480\n",
            "1117:\t-1.047777295\n",
            "1118:\t-1.046626210\n",
            "1119:\t-0.959568977\n",
            "1120:\t-0.860544086\n",
            "1121:\t-1.005578756\n",
            "1122:\t-0.986278057\n",
            "1123:\t-0.897034764\n",
            "1124:\t-1.089286566\n",
            "1125:\t-1.003419757\n",
            "1126:\t-0.927425146\n",
            "1127:\t-1.028306961\n",
            "1128:\t-0.872284710\n",
            "1129:\t-0.918625355\n",
            "1130:\t-0.885597467\n",
            "1131:\t-1.067222476\n",
            "1132:\t-0.942085981\n",
            "1133:\t-0.847739220\n",
            "1134:\t-0.909880221\n",
            "1135:\t-1.040686846\n",
            "1136:\t-1.055912375\n",
            "1137:\t-1.002081156\n",
            "1138:\t-0.855786085\n",
            "1139:\t-0.901723385\n",
            "1140:\t-0.996830404\n",
            "1141:\t-0.885522544\n",
            "1142:\t-0.932798445\n",
            "1143:\t-0.936535299\n",
            "1144:\t-0.839980900\n",
            "1145:\t-1.085860968\n",
            "1146:\t-0.951287389\n",
            "1147:\t-0.881243169\n",
            "1148:\t-0.972198427\n",
            "1149:\t-0.913705051\n",
            "1150:\t-0.938751519\n",
            "1151:\t-0.971045256\n",
            "1152:\t-0.992039323\n",
            "1153:\t-1.013582945\n",
            "1154:\t-0.955970049\n",
            "1155:\t-0.989162564\n",
            "1156:\t-1.008977413\n",
            "1157:\t-0.907077610\n",
            "1158:\t-0.918832839\n",
            "1159:\t-0.897726357\n",
            "1160:\t-0.943914175\n",
            "1161:\t-1.008052349\n",
            "1162:\t-1.084230423\n",
            "1163:\t-0.939387143\n",
            "1164:\t-1.038851976\n",
            "1165:\t-1.115586877\n",
            "1166:\t-0.978522003\n",
            "1167:\t-0.955652595\n",
            "1168:\t-0.863463819\n",
            "1169:\t-0.975659966\n",
            "1170:\t-0.985716105\n",
            "1171:\t-0.960853636\n",
            "1172:\t-0.986342609\n",
            "1173:\t-0.885266840\n",
            "1174:\t-0.880289733\n",
            "1175:\t-0.961001992\n",
            "1176:\t-0.896742761\n",
            "1177:\t-0.986488640\n",
            "1178:\t-1.072551966\n",
            "1179:\t-0.917732835\n",
            "1180:\t-0.907389283\n",
            "1181:\t-0.889961958\n",
            "1182:\t-0.890636802\n",
            "1183:\t-0.888869226\n",
            "1184:\t-0.931839287\n",
            "1185:\t-0.901957810\n",
            "1186:\t-1.058864236\n",
            "1187:\t-0.908607781\n",
            "1188:\t-0.887128234\n",
            "1189:\t-0.927007377\n",
            "1190:\t-0.962565362\n",
            "1191:\t-0.901698291\n",
            "1192:\t-0.911721468\n",
            "1193:\t-0.960316122\n",
            "1194:\t-0.884927154\n",
            "1195:\t-0.937343478\n",
            "1196:\t-0.915499151\n",
            "1197:\t-0.956868470\n",
            "1198:\t-0.971870542\n",
            "1199:\t-0.923710883\n",
            "1200:\t-1.026091337\n",
            "1201:\t-0.990518868\n",
            "1202:\t-0.987521589\n",
            "1203:\t-0.991820872\n",
            "1204:\t-0.973428965\n",
            "1205:\t-0.983660936\n",
            "1206:\t-0.886215806\n",
            "1207:\t-0.970597923\n",
            "1208:\t-0.984269977\n",
            "1209:\t-0.938491464\n",
            "1210:\t-0.990881026\n",
            "1211:\t-0.967384696\n",
            "1212:\t-0.960224152\n",
            "1213:\t-0.922145367\n",
            "1214:\t-0.891163051\n",
            "1215:\t-1.061663866\n",
            "1216:\t-1.078527808\n",
            "1217:\t-0.884216428\n",
            "1218:\t-0.894348741\n",
            "1219:\t-0.891908705\n",
            "1220:\t-0.867604017\n",
            "1221:\t-1.126205921\n",
            "1222:\t-0.879159570\n",
            "1223:\t-0.880181491\n",
            "1224:\t-0.959104836\n",
            "1225:\t-0.930966496\n",
            "1226:\t-0.917591870\n",
            "1227:\t-0.897706091\n",
            "1228:\t-1.082613826\n",
            "1229:\t-1.039465785\n",
            "1230:\t-0.931885719\n",
            "1231:\t-1.097220898\n",
            "1232:\t-0.881856978\n",
            "1233:\t-0.977239490\n",
            "1234:\t-1.024162769\n",
            "1235:\t-0.954589248\n",
            "1236:\t-1.038879395\n",
            "1237:\t-0.990242541\n",
            "1238:\t-0.898929417\n",
            "1239:\t-0.967584789\n",
            "1240:\t-1.050561428\n",
            "1241:\t-0.912780583\n",
            "1242:\t-0.923353910\n",
            "1243:\t-0.978333712\n",
            "1244:\t-1.055415869\n",
            "1245:\t-0.998423636\n",
            "1246:\t-1.019624114\n",
            "1247:\t-1.049768329\n",
            "1248:\t-0.993536949\n",
            "1249:\t-1.158506393\n",
            "1250:\t-1.035665393\n",
            "1251:\t-0.920184731\n",
            "1252:\t-0.919039071\n",
            "1253:\t-0.923731208\n",
            "1254:\t-0.889089346\n",
            "1255:\t-0.949167669\n",
            "1256:\t-0.876367986\n",
            "1257:\t-0.938461661\n",
            "1258:\t-0.874198377\n",
            "1259:\t-0.952858329\n",
            "1260:\t-1.010251880\n",
            "1261:\t-0.915432870\n",
            "1262:\t-0.866940320\n",
            "1263:\t-0.891926110\n",
            "1264:\t-0.877364814\n",
            "1265:\t-0.908368886\n",
            "1266:\t-0.868102729\n",
            "1267:\t-0.919250607\n",
            "1268:\t-0.883505583\n",
            "1269:\t-0.893445075\n",
            "1270:\t-0.905150771\n",
            "1271:\t-1.060374618\n",
            "1272:\t-0.899081409\n",
            "1273:\t-0.901923776\n",
            "1274:\t-0.900431573\n",
            "1275:\t-0.908747852\n",
            "1276:\t-1.000039935\n",
            "1277:\t-0.894375086\n",
            "1278:\t-0.967567503\n",
            "1279:\t-0.881357551\n",
            "1280:\t-0.987195015\n",
            "1281:\t-0.974384010\n",
            "1282:\t-1.024874449\n",
            "1283:\t-0.915991187\n",
            "1284:\t-0.942886174\n",
            "1285:\t-1.029929876\n",
            "1286:\t-1.031236649\n",
            "1287:\t-1.002251625\n",
            "1288:\t-0.910678506\n",
            "1289:\t-0.891929507\n",
            "1290:\t-0.964984655\n",
            "1291:\t-1.012110114\n",
            "1292:\t-0.967532992\n",
            "1293:\t-0.863323808\n",
            "1294:\t-0.892567039\n",
            "1295:\t-0.969127357\n",
            "1296:\t-0.914334595\n",
            "1297:\t-0.899627209\n",
            "1298:\t-1.122975945\n",
            "1299:\t-0.894281685\n",
            "1300:\t-0.971681178\n",
            "1301:\t-1.103091359\n",
            "1302:\t-0.949191928\n",
            "1303:\t-0.924372256\n",
            "1304:\t-1.117879748\n",
            "1305:\t-1.012561798\n",
            "1306:\t-1.102198720\n",
            "1307:\t-0.933295608\n",
            "1308:\t-1.050421357\n",
            "1309:\t-0.869802594\n",
            "1310:\t-1.089753151\n",
            "1311:\t-0.949941576\n",
            "1312:\t-0.907846749\n",
            "1313:\t-1.029682040\n",
            "1314:\t-1.008618116\n",
            "1315:\t-1.043189287\n",
            "1316:\t-0.952194214\n",
            "1317:\t-0.942718685\n",
            "1318:\t-1.065264344\n",
            "1319:\t-1.007240176\n",
            "1320:\t-0.921685994\n",
            "1321:\t-1.055584669\n",
            "1322:\t-0.974109054\n",
            "1323:\t-0.871758521\n",
            "1324:\t-0.898744524\n",
            "1325:\t-0.957624674\n",
            "1326:\t-0.870756745\n",
            "1327:\t-0.860851228\n",
            "1328:\t-0.908053994\n",
            "1329:\t-0.909045696\n",
            "1330:\t-0.890137732\n",
            "1331:\t-0.865344584\n",
            "1332:\t-0.884282470\n",
            "1333:\t-1.015893698\n",
            "1334:\t-1.056594491\n",
            "1335:\t-0.992879570\n",
            "1336:\t-1.119688034\n",
            "1337:\t-0.946075141\n",
            "1338:\t-1.012858629\n",
            "1339:\t-0.978376806\n",
            "1340:\t-0.860854983\n",
            "1341:\t-0.915473759\n",
            "1342:\t-0.913888216\n",
            "1343:\t-0.998813331\n",
            "1344:\t-0.889261305\n",
            "1345:\t-0.894297481\n",
            "1346:\t-1.092788219\n",
            "1347:\t-0.902269006\n",
            "1348:\t-1.029336214\n",
            "1349:\t-0.927580118\n",
            "1350:\t-0.884741545\n",
            "1351:\t-0.918886602\n",
            "1352:\t-0.934423029\n",
            "1353:\t-0.890389621\n",
            "1354:\t-1.068381310\n",
            "1355:\t-0.980919957\n",
            "1356:\t-0.901097834\n",
            "1357:\t-0.986972451\n",
            "1358:\t-0.899977326\n",
            "1359:\t-0.962436914\n",
            "1360:\t-1.138103604\n",
            "1361:\t-0.979475975\n",
            "1362:\t-0.926283121\n",
            "1363:\t-0.850661695\n",
            "1364:\t-0.995570898\n",
            "1365:\t-1.092516184\n",
            "1366:\t-0.973631799\n",
            "1367:\t-0.917839050\n",
            "1368:\t-0.919168890\n",
            "1369:\t-0.888342381\n",
            "1370:\t-0.892548501\n",
            "1371:\t-0.946585655\n",
            "1372:\t-0.923754990\n",
            "1373:\t-0.900581777\n",
            "1374:\t-0.967462063\n",
            "1375:\t-0.967729092\n",
            "1376:\t-1.007608056\n",
            "1377:\t-0.930028081\n",
            "1378:\t-0.974529207\n",
            "1379:\t-1.085129261\n",
            "1380:\t-0.956303298\n",
            "1381:\t-0.918542266\n",
            "1382:\t-1.054948092\n",
            "1383:\t-0.940481842\n",
            "Epoch: 3\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "1384:\t-0.914810002\n",
            "1385:\t-0.999787986\n",
            "1386:\t-0.878553391\n",
            "1387:\t-0.909769833\n",
            "1388:\t-0.912133753\n",
            "1389:\t-1.100854397\n",
            "1390:\t-0.993401885\n",
            "1391:\t-0.907927990\n",
            "1392:\t-1.117437720\n",
            "1393:\t-0.932861805\n",
            "1394:\t-0.897907972\n",
            "1395:\t-0.921555579\n",
            "1396:\t-0.975026786\n",
            "1397:\t-0.969413400\n",
            "1398:\t-1.054682374\n",
            "1399:\t-0.986894488\n",
            "1400:\t-0.946493268\n",
            "1401:\t-0.953808367\n",
            "1402:\t-1.051544070\n",
            "1403:\t-1.007986426\n",
            "1404:\t-1.081081986\n",
            "1405:\t-1.044057608\n",
            "1406:\t-0.997037709\n",
            "1407:\t-0.981654525\n",
            "1408:\t-1.040188789\n",
            "1409:\t-0.922028124\n",
            "1410:\t-1.001888752\n",
            "1411:\t-0.927896380\n",
            "1412:\t-0.965402544\n",
            "1413:\t-0.907919526\n",
            "1414:\t-0.981310427\n",
            "1415:\t-0.881576240\n",
            "1416:\t-1.067957401\n",
            "1417:\t-0.933414757\n",
            "1418:\t-1.077644825\n",
            "1419:\t-1.131316662\n",
            "1420:\t-0.896101594\n",
            "1421:\t-1.112269402\n",
            "1422:\t-0.909847558\n",
            "1423:\t-0.873914838\n",
            "1424:\t-0.902003229\n",
            "1425:\t-0.895289838\n",
            "1426:\t-0.934043646\n",
            "1427:\t-0.929669559\n",
            "1428:\t-0.850667477\n",
            "1429:\t-0.925629139\n",
            "1430:\t-0.900668323\n",
            "1431:\t-0.960740745\n",
            "1432:\t-1.109786034\n",
            "1433:\t-0.903431475\n",
            "1434:\t-0.901671648\n",
            "1435:\t-0.926572382\n",
            "1436:\t-0.866580129\n",
            "1437:\t-0.934300303\n",
            "1438:\t-0.898583531\n",
            "1439:\t-0.898453057\n",
            "1440:\t-0.900036395\n",
            "1441:\t-0.962239683\n",
            "1442:\t-1.015469074\n",
            "1443:\t-0.989950180\n",
            "1444:\t-0.893812597\n",
            "1445:\t-0.915291607\n",
            "1446:\t-0.919678867\n",
            "1447:\t-0.885011315\n",
            "1448:\t-0.944642782\n",
            "1449:\t-0.916874051\n",
            "1450:\t-0.913505912\n",
            "1451:\t-1.058846354\n",
            "1452:\t-0.925528944\n",
            "1453:\t-1.017837405\n",
            "1454:\t-0.904858112\n",
            "1455:\t-0.971345067\n",
            "1456:\t-1.040173650\n",
            "1457:\t-0.995700061\n",
            "1458:\t-0.987393022\n",
            "1459:\t-0.898685455\n",
            "1460:\t-0.896508813\n",
            "1461:\t-1.009005308\n",
            "1462:\t-0.918979049\n",
            "1463:\t-0.910859287\n",
            "1464:\t-0.914527833\n",
            "1465:\t-1.079807878\n",
            "1466:\t-0.907908082\n",
            "1467:\t-0.904556394\n",
            "1468:\t-0.918822229\n",
            "1469:\t-1.052539229\n",
            "1470:\t-0.912026823\n",
            "1471:\t-0.951381385\n",
            "1472:\t-0.921146274\n",
            "1473:\t-0.919096828\n",
            "1474:\t-1.044777036\n",
            "1475:\t-0.933897078\n",
            "1476:\t-1.070258856\n",
            "1477:\t-1.036423445\n",
            "1478:\t-0.885934830\n",
            "1479:\t-0.946767867\n",
            "1480:\t-0.923808157\n",
            "1481:\t-0.915053248\n",
            "1482:\t-1.068718791\n",
            "1483:\t-0.978249490\n",
            "1484:\t-1.025851369\n",
            "1485:\t-1.156429410\n",
            "1486:\t-0.907055557\n",
            "1487:\t-0.876445889\n",
            "1488:\t-0.886957288\n",
            "1489:\t-1.095422745\n",
            "1490:\t-0.888880789\n",
            "1491:\t-0.998754084\n",
            "1492:\t-0.970212340\n",
            "1493:\t-0.973276198\n",
            "1494:\t-0.982824445\n",
            "1495:\t-0.996064186\n",
            "1496:\t-1.062072515\n",
            "1497:\t-1.000433445\n",
            "1498:\t-0.921586096\n",
            "1499:\t-0.907675445\n",
            "1500:\t-0.917684078\n",
            "Saving model and optimizer state at iteration 1500 to outdir/model_1500\n",
            "1501:\t-1.073179007\n",
            "1502:\t-0.887544870\n",
            "1503:\t-0.943959475\n",
            "1504:\t-0.966604114\n",
            "1505:\t-0.916128159\n",
            "1506:\t-0.997510195\n",
            "1507:\t-0.996418118\n",
            "1508:\t-0.953000665\n",
            "1509:\t-0.893468142\n",
            "1510:\t-0.885009170\n",
            "1511:\t-0.945847273\n",
            "1512:\t-0.884153128\n",
            "1513:\t-1.119120479\n",
            "1514:\t-0.975299954\n",
            "1515:\t-0.881983817\n",
            "1516:\t-1.082463384\n",
            "1517:\t-1.105441093\n",
            "1518:\t-0.969051182\n",
            "1519:\t-1.058160782\n",
            "1520:\t-1.007306457\n",
            "1521:\t-0.885445058\n",
            "1522:\t-0.920368493\n",
            "1523:\t-0.980005860\n",
            "1524:\t-0.912125230\n",
            "1525:\t-0.924044967\n",
            "1526:\t-0.885052979\n",
            "1527:\t-0.973080575\n",
            "1528:\t-1.017943501\n",
            "1529:\t-0.936776698\n",
            "1530:\t-0.960900784\n",
            "1531:\t-1.061759472\n",
            "1532:\t-0.916373789\n",
            "1533:\t-0.974997044\n",
            "1534:\t-1.035246611\n",
            "1535:\t-1.102142334\n",
            "1536:\t-1.133451223\n",
            "1537:\t-1.099238873\n",
            "1538:\t-1.067897439\n",
            "1539:\t-0.901773930\n",
            "1540:\t-0.996745527\n",
            "1541:\t-1.010259271\n",
            "1542:\t-1.036005497\n",
            "1543:\t-0.919105351\n",
            "1544:\t-0.980699182\n",
            "1545:\t-1.005840778\n",
            "1546:\t-0.942090750\n",
            "1547:\t-0.969475329\n",
            "1548:\t-0.908014357\n",
            "1549:\t-0.953060567\n",
            "1550:\t-0.963383734\n",
            "1551:\t-0.863984346\n",
            "1552:\t-0.999387205\n",
            "1553:\t-0.957914352\n",
            "1554:\t-0.919651687\n",
            "1555:\t-1.062788844\n",
            "1556:\t-0.924462736\n",
            "1557:\t-1.069943666\n",
            "1558:\t-0.899658263\n",
            "1559:\t-1.004031062\n",
            "1560:\t-0.786180317\n",
            "1561:\t-1.000395894\n",
            "1562:\t-1.056480527\n",
            "1563:\t-1.078743815\n",
            "1564:\t-1.098198414\n",
            "1565:\t-1.010425925\n",
            "1566:\t-0.989271224\n",
            "1567:\t-0.966686547\n",
            "1568:\t-0.897100091\n",
            "1569:\t-0.860919356\n",
            "1570:\t-0.924307048\n",
            "1571:\t-0.876123250\n",
            "1572:\t-1.076399446\n",
            "1573:\t-1.009084344\n",
            "1574:\t-0.856890261\n",
            "1575:\t-0.902469635\n",
            "1576:\t-0.898818254\n",
            "1577:\t-0.997234643\n",
            "1578:\t-1.039140940\n",
            "1579:\t-0.998633325\n",
            "1580:\t-1.025819182\n",
            "1581:\t-1.002373815\n",
            "1582:\t-0.912358880\n",
            "1583:\t-0.854869962\n",
            "1584:\t-0.889963686\n",
            "1585:\t-0.891298413\n",
            "1586:\t-1.006770730\n",
            "1587:\t-1.055077434\n",
            "1588:\t-0.963925779\n",
            "1589:\t-0.957195222\n",
            "1590:\t-0.927335739\n",
            "1591:\t-0.950203657\n",
            "1592:\t-0.906415999\n",
            "1593:\t-1.016080737\n",
            "1594:\t-1.013056278\n",
            "1595:\t-0.924071372\n",
            "1596:\t-0.899673104\n",
            "1597:\t-0.898081481\n",
            "1598:\t-1.093592763\n",
            "1599:\t-0.928230405\n",
            "1600:\t-1.030957341\n",
            "1601:\t-1.006665707\n",
            "1602:\t-1.015600204\n",
            "1603:\t-0.881389558\n",
            "1604:\t-0.926266849\n",
            "1605:\t-0.970266581\n",
            "1606:\t-0.939262629\n",
            "1607:\t-1.060263634\n",
            "1608:\t-0.928574920\n",
            "1609:\t-1.084643602\n",
            "1610:\t-0.924302578\n",
            "1611:\t-0.971466124\n",
            "1612:\t-0.994644761\n",
            "1613:\t-0.939574778\n",
            "1614:\t-1.010035038\n",
            "1615:\t-1.087288499\n",
            "1616:\t-0.991424441\n",
            "1617:\t-1.021024585\n",
            "1618:\t-1.070414424\n",
            "1619:\t-0.899675012\n",
            "1620:\t-0.962400913\n",
            "1621:\t-0.918546200\n",
            "1622:\t-1.086528897\n",
            "1623:\t-1.046369672\n",
            "1624:\t-1.090536714\n",
            "1625:\t-0.999484003\n",
            "1626:\t-1.118850470\n",
            "1627:\t-0.909082711\n",
            "1628:\t-1.065784454\n",
            "1629:\t-1.007520199\n",
            "1630:\t-1.028002739\n",
            "1631:\t-0.856104553\n",
            "1632:\t-0.853405416\n",
            "1633:\t-1.051111341\n",
            "1634:\t-0.871478617\n",
            "1635:\t-0.831618249\n",
            "1636:\t-0.938443601\n",
            "1637:\t-1.035866261\n",
            "1638:\t-1.116214156\n",
            "1639:\t-0.875544965\n",
            "1640:\t-0.950984299\n",
            "1641:\t-0.886324286\n",
            "1642:\t-1.000094771\n",
            "1643:\t-0.973132908\n",
            "1644:\t-0.859014034\n",
            "1645:\t-0.850459039\n",
            "1646:\t-0.988784254\n",
            "1647:\t-0.895602286\n",
            "1648:\t-0.878783703\n",
            "1649:\t-0.851434708\n",
            "1650:\t-0.924952090\n",
            "1651:\t-0.877437651\n",
            "1652:\t-0.995299816\n",
            "1653:\t-0.900540829\n",
            "1654:\t-1.001098514\n",
            "1655:\t-0.956356764\n",
            "1656:\t-0.985820889\n",
            "1657:\t-0.904430330\n",
            "1658:\t-0.914143801\n",
            "1659:\t-1.002743483\n",
            "1660:\t-1.013947964\n",
            "1661:\t-0.928725183\n",
            "1662:\t-0.947117448\n",
            "1663:\t-0.923617601\n",
            "1664:\t-0.918896616\n",
            "1665:\t-1.020896554\n",
            "1666:\t-0.850467384\n",
            "1667:\t-0.987828732\n",
            "1668:\t-1.028942466\n",
            "1669:\t-0.994464755\n",
            "1670:\t-0.947004020\n",
            "1671:\t-0.973052680\n",
            "1672:\t-0.954212844\n",
            "1673:\t-0.984814644\n",
            "1674:\t-0.981500864\n",
            "1675:\t-0.909676015\n",
            "1676:\t-1.047186852\n",
            "1677:\t-0.963555157\n",
            "1678:\t-0.973460436\n",
            "1679:\t-0.981853366\n",
            "1680:\t-0.953564465\n",
            "1681:\t-1.028242230\n",
            "1682:\t-0.976642668\n",
            "1683:\t-1.012466788\n",
            "1684:\t-0.928292096\n",
            "1685:\t-0.870199561\n",
            "1686:\t-1.068464041\n",
            "1687:\t-1.120134592\n",
            "1688:\t-1.006738305\n",
            "1689:\t-0.976099312\n",
            "1690:\t-0.922414601\n",
            "1691:\t-1.109597921\n",
            "1692:\t-0.885216475\n",
            "1693:\t-0.902776062\n",
            "1694:\t-1.121506572\n",
            "1695:\t-0.919431508\n",
            "1696:\t-1.080225945\n",
            "1697:\t-0.872553349\n",
            "1698:\t-0.877648413\n",
            "1699:\t-0.993217885\n",
            "1700:\t-0.868659019\n",
            "1701:\t-0.954883158\n",
            "1702:\t-1.008291721\n",
            "1703:\t-1.037220240\n",
            "1704:\t-1.066903949\n",
            "1705:\t-1.021042824\n",
            "1706:\t-1.130457997\n",
            "1707:\t-0.903818488\n",
            "1708:\t-1.088634968\n",
            "1709:\t-0.904444039\n",
            "1710:\t-1.196770072\n",
            "1711:\t-0.896221817\n",
            "1712:\t-0.942963541\n",
            "1713:\t-1.072265387\n",
            "1714:\t-0.911753893\n",
            "1715:\t-0.903710365\n",
            "1716:\t-0.890514255\n",
            "1717:\t-0.869295776\n",
            "1718:\t-0.964326441\n",
            "1719:\t-1.082269549\n",
            "1720:\t-0.889864683\n",
            "1721:\t-0.928216338\n",
            "1722:\t-0.954011500\n",
            "1723:\t-0.899787307\n",
            "1724:\t-0.873011291\n",
            "1725:\t-0.983598292\n",
            "1726:\t-0.901898801\n",
            "1727:\t-0.993042946\n",
            "1728:\t-0.945558429\n",
            "1729:\t-1.037871242\n",
            "1730:\t-0.933835447\n",
            "1731:\t-1.006136537\n",
            "1732:\t-0.924945235\n",
            "1733:\t-1.090114117\n",
            "1734:\t-1.014257073\n",
            "1735:\t-0.860392511\n",
            "1736:\t-0.904292464\n",
            "1737:\t-0.913867056\n",
            "1738:\t-0.889407516\n",
            "1739:\t-0.871418297\n",
            "1740:\t-0.909983397\n",
            "1741:\t-0.943810284\n",
            "1742:\t-0.972467422\n",
            "1743:\t-0.931442440\n",
            "1744:\t-0.986827493\n",
            "1745:\t-1.046907425\n",
            "1746:\t-0.866381943\n",
            "1747:\t-1.034190893\n",
            "1748:\t-1.024930239\n",
            "1749:\t-1.057647347\n",
            "1750:\t-1.044513106\n",
            "1751:\t-1.039726734\n",
            "1752:\t-1.013937116\n",
            "1753:\t-0.951493919\n",
            "1754:\t-0.972956300\n",
            "1755:\t-0.897961199\n",
            "1756:\t-0.924122155\n",
            "1757:\t-0.889934182\n",
            "1758:\t-0.951907277\n",
            "1759:\t-1.046882510\n",
            "1760:\t-0.976330519\n",
            "1761:\t-0.904859483\n",
            "1762:\t-0.999132037\n",
            "1763:\t-1.014378786\n",
            "1764:\t-0.920243263\n",
            "1765:\t-0.888856828\n",
            "1766:\t-1.006715655\n",
            "1767:\t-1.015718937\n",
            "1768:\t-1.035177827\n",
            "1769:\t-1.112802982\n",
            "1770:\t-1.058899403\n",
            "1771:\t-0.900667667\n",
            "1772:\t-1.061745882\n",
            "1773:\t-1.019635797\n",
            "1774:\t-0.954499662\n",
            "1775:\t-1.048132896\n",
            "1776:\t-0.910166323\n",
            "1777:\t-1.047230244\n",
            "1778:\t-1.090930223\n",
            "1779:\t-1.021560907\n",
            "1780:\t-0.960459828\n",
            "1781:\t-1.101915359\n",
            "1782:\t-0.932298303\n",
            "1783:\t-1.081880450\n",
            "1784:\t-0.963242829\n",
            "1785:\t-0.895784676\n",
            "1786:\t-1.099485755\n",
            "1787:\t-0.825548649\n",
            "1788:\t-0.901782751\n",
            "1789:\t-0.913715422\n",
            "1790:\t-0.872186422\n",
            "1791:\t-0.873569608\n",
            "1792:\t-0.990266263\n",
            "1793:\t-0.991687953\n",
            "1794:\t-0.867210865\n",
            "1795:\t-0.895175457\n",
            "1796:\t-0.943299413\n",
            "1797:\t-1.004047394\n",
            "1798:\t-1.021385670\n",
            "1799:\t-0.903612494\n",
            "1800:\t-0.898292243\n",
            "1801:\t-0.906888306\n",
            "1802:\t-0.886160672\n",
            "1803:\t-1.043468118\n",
            "1804:\t-0.995181262\n",
            "1805:\t-1.008332610\n",
            "1806:\t-1.020941138\n",
            "1807:\t-0.977865219\n",
            "1808:\t-0.938010931\n",
            "1809:\t-0.943921626\n",
            "1810:\t-0.998012066\n",
            "1811:\t-1.036899805\n",
            "1812:\t-0.873424768\n",
            "1813:\t-0.956375420\n",
            "1814:\t-0.867146015\n",
            "1815:\t-1.017680287\n",
            "1816:\t-0.985675097\n",
            "1817:\t-0.953440666\n",
            "1818:\t-0.952966452\n",
            "1819:\t-1.015690446\n",
            "1820:\t-1.026592493\n",
            "1821:\t-1.077568531\n",
            "1822:\t-0.907679260\n",
            "1823:\t-0.924662769\n",
            "1824:\t-0.930674553\n",
            "1825:\t-0.966789365\n",
            "1826:\t-0.905932307\n",
            "1827:\t-0.862196267\n",
            "1828:\t-0.934318602\n",
            "1829:\t-0.919636667\n",
            "1830:\t-1.087365270\n",
            "1831:\t-0.934511483\n",
            "1832:\t-1.047647119\n",
            "1833:\t-0.932796597\n",
            "1834:\t-0.967197835\n",
            "1835:\t-0.939483702\n",
            "1836:\t-0.890412867\n",
            "1837:\t-1.031222701\n",
            "1838:\t-1.003010869\n",
            "1839:\t-0.877651274\n",
            "1840:\t-1.066369534\n",
            "1841:\t-0.991556227\n",
            "1842:\t-0.892486811\n",
            "1843:\t-0.932071865\n",
            "1844:\t-0.922714889\n",
            "Epoch: 4\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "1845:\t-0.901914835\n",
            "1846:\t-0.989581466\n",
            "1847:\t-1.052735806\n",
            "1848:\t-1.007195234\n",
            "1849:\t-0.900371552\n",
            "1850:\t-0.934434712\n",
            "1851:\t-1.038224459\n",
            "1852:\t-1.042594314\n",
            "1853:\t-1.003995419\n",
            "1854:\t-1.060316682\n",
            "1855:\t-0.934348106\n",
            "1856:\t-0.909654558\n",
            "1857:\t-1.028705359\n",
            "1858:\t-0.961841345\n",
            "1859:\t-0.904861629\n",
            "1860:\t-0.962871075\n",
            "1861:\t-1.071152091\n",
            "1862:\t-0.899366736\n",
            "1863:\t-1.016837716\n",
            "1864:\t-1.022531033\n",
            "1865:\t-1.026168823\n",
            "1866:\t-1.147880673\n",
            "1867:\t-1.044420838\n",
            "1868:\t-0.960822284\n",
            "1869:\t-0.907362759\n",
            "1870:\t-0.980794668\n",
            "1871:\t-1.103225708\n",
            "1872:\t-0.984657288\n",
            "1873:\t-0.994500935\n",
            "1874:\t-1.136327147\n",
            "1875:\t-0.914573371\n",
            "1876:\t-0.921017766\n",
            "1877:\t-0.942217946\n",
            "1878:\t-0.901270211\n",
            "1879:\t-1.093405724\n",
            "1880:\t-0.916856945\n",
            "1881:\t-0.960110068\n",
            "1882:\t-0.959027231\n",
            "1883:\t-0.907629490\n",
            "1884:\t-0.928580940\n",
            "1885:\t-1.099081874\n",
            "1886:\t-0.928857982\n",
            "1887:\t-1.024006486\n",
            "1888:\t-1.043201327\n",
            "1889:\t-0.881730437\n",
            "1890:\t-0.949204326\n",
            "1891:\t-1.080988050\n",
            "1892:\t-0.910620987\n",
            "1893:\t-0.916848242\n",
            "1894:\t-1.054550886\n",
            "1895:\t-0.883333802\n",
            "1896:\t-0.910311162\n",
            "1897:\t-0.938294649\n",
            "1898:\t-1.024035454\n",
            "1899:\t-1.054534078\n",
            "1900:\t-1.004299760\n",
            "1901:\t-0.959845245\n",
            "1902:\t-0.992360592\n",
            "1903:\t-0.925234079\n",
            "1904:\t-0.951329947\n",
            "1905:\t-1.136638641\n",
            "1906:\t-0.893876553\n",
            "1907:\t-1.022738934\n",
            "1908:\t-1.088121891\n",
            "1909:\t-1.024252772\n",
            "1910:\t-1.008622050\n",
            "1911:\t-1.103367567\n",
            "1912:\t-0.918346107\n",
            "1913:\t-0.989901006\n",
            "1914:\t-0.931647420\n",
            "1915:\t-1.007846236\n",
            "1916:\t-0.893221378\n",
            "1917:\t-1.084115744\n",
            "1918:\t-0.907558620\n",
            "1919:\t-0.950887382\n",
            "1920:\t-0.981587112\n",
            "1921:\t-1.062702179\n",
            "1922:\t-0.952734590\n",
            "1923:\t-0.908147812\n",
            "1924:\t-0.882898390\n",
            "1925:\t-0.899250686\n",
            "1926:\t-0.953338623\n",
            "1927:\t-0.910315096\n",
            "1928:\t-0.909817398\n",
            "1929:\t-1.072012663\n",
            "1930:\t-0.905602276\n",
            "1931:\t-1.060211897\n",
            "1932:\t-1.117836237\n",
            "1933:\t-0.928252220\n",
            "1934:\t-1.017134666\n",
            "1935:\t-0.952096343\n",
            "1936:\t-0.932954013\n",
            "1937:\t-0.993421793\n",
            "1938:\t-1.054893017\n",
            "1939:\t-1.062967420\n",
            "1940:\t-1.089841008\n",
            "1941:\t-0.893909752\n",
            "1942:\t-1.142755151\n",
            "1943:\t-0.898008049\n",
            "1944:\t-0.980445743\n",
            "1945:\t-0.899810791\n",
            "1946:\t-0.981339872\n",
            "1947:\t-0.912059367\n",
            "1948:\t-1.080663204\n",
            "1949:\t-0.972984076\n",
            "1950:\t-1.123297811\n",
            "1951:\t-0.905853212\n",
            "1952:\t-0.893300116\n",
            "1953:\t-0.998592377\n",
            "1954:\t-1.024394870\n",
            "1955:\t-0.890363216\n",
            "1956:\t-0.901425540\n",
            "1957:\t-1.095237255\n",
            "1958:\t-0.895919502\n",
            "1959:\t-0.898089230\n",
            "1960:\t-1.119611502\n",
            "1961:\t-0.904691517\n",
            "1962:\t-1.003794193\n",
            "1963:\t-1.035817981\n",
            "1964:\t-0.992269218\n",
            "1965:\t-0.979745686\n",
            "1966:\t-1.020758748\n",
            "1967:\t-0.877608001\n",
            "1968:\t-1.024829507\n",
            "1969:\t-0.891694903\n",
            "1970:\t-0.874019206\n",
            "1971:\t-0.905165553\n",
            "1972:\t-0.893832386\n",
            "1973:\t-0.893275082\n",
            "1974:\t-0.932920516\n",
            "1975:\t-0.947412670\n",
            "1976:\t-1.048061728\n",
            "1977:\t-0.885162592\n",
            "1978:\t-0.917947412\n",
            "1979:\t-0.991492271\n",
            "1980:\t-1.116101503\n",
            "1981:\t-1.006501675\n",
            "1982:\t-0.866457582\n",
            "1983:\t-0.972627997\n",
            "1984:\t-0.885072231\n",
            "1985:\t-0.927608192\n",
            "1986:\t-0.923335016\n",
            "1987:\t-0.869157374\n",
            "1988:\t-0.975740612\n",
            "1989:\t-0.908409894\n",
            "1990:\t-1.032020569\n",
            "1991:\t-0.936693430\n",
            "1992:\t-1.009674549\n",
            "1993:\t-0.973859012\n",
            "1994:\t-1.078674078\n",
            "1995:\t-0.918381274\n",
            "1996:\t-0.906651497\n",
            "1997:\t-0.998392820\n",
            "1998:\t-0.883233666\n",
            "1999:\t-1.055408835\n",
            "2000:\t-0.884383976\n",
            "Saving model and optimizer state at iteration 2000 to outdir/model_2000\n",
            "2001:\t-0.901206315\n",
            "2002:\t-0.867630601\n",
            "2003:\t-0.876203001\n",
            "2004:\t-1.008445263\n",
            "2005:\t-1.039384961\n",
            "2006:\t-0.974763393\n",
            "2007:\t-0.965240717\n",
            "2008:\t-0.909490228\n",
            "2009:\t-0.925748229\n",
            "2010:\t-0.915203035\n",
            "2011:\t-0.930088758\n",
            "2012:\t-1.076307297\n",
            "2013:\t-0.879108548\n",
            "2014:\t-1.086194396\n",
            "2015:\t-0.992433608\n",
            "2016:\t-0.980129719\n",
            "2017:\t-0.887625396\n",
            "2018:\t-1.034649611\n",
            "2019:\t-0.941658258\n",
            "2020:\t-1.132320523\n",
            "2021:\t-1.078219175\n",
            "2022:\t-0.923846722\n",
            "2023:\t-1.076778173\n",
            "2024:\t-1.019068718\n",
            "2025:\t-0.885261178\n",
            "2026:\t-0.975760221\n",
            "2027:\t-0.891781747\n",
            "2028:\t-1.113329172\n",
            "2029:\t-0.882674813\n",
            "2030:\t-1.031816125\n",
            "2031:\t-0.921405315\n",
            "2032:\t-0.940088391\n",
            "2033:\t-1.091471910\n",
            "2034:\t-0.997526646\n",
            "2035:\t-0.862467170\n",
            "2036:\t-0.992815435\n",
            "2037:\t-1.050021648\n",
            "2038:\t-0.885121167\n",
            "2039:\t-0.899229586\n",
            "2040:\t-0.948237777\n",
            "2041:\t-0.908772767\n",
            "2042:\t-0.969736159\n",
            "2043:\t-0.893278301\n",
            "2044:\t-0.904917300\n",
            "2045:\t-1.047999024\n",
            "2046:\t-1.115286827\n",
            "2047:\t-1.088660121\n",
            "2048:\t-0.918453276\n",
            "2049:\t-1.021776438\n",
            "2050:\t-1.022601247\n",
            "2051:\t-0.888587177\n",
            "2052:\t-1.170359731\n",
            "2053:\t-0.867705643\n",
            "2054:\t-1.121290088\n",
            "2055:\t-0.896163881\n",
            "2056:\t-0.975571871\n",
            "2057:\t-1.045358181\n",
            "2058:\t-0.958014846\n",
            "2059:\t-0.921596229\n",
            "2060:\t-0.891645432\n",
            "2061:\t-0.989037156\n",
            "2062:\t-1.005843163\n",
            "2063:\t-1.044938684\n",
            "2064:\t-0.909563780\n",
            "2065:\t-1.017802477\n",
            "2066:\t-1.055776954\n",
            "2067:\t-1.014190078\n",
            "2068:\t-0.931002855\n",
            "2069:\t-1.034851074\n",
            "2070:\t-1.092238545\n",
            "2071:\t-1.064637899\n",
            "2072:\t-0.921210110\n",
            "2073:\t-0.902985990\n",
            "2074:\t-1.031017900\n",
            "2075:\t-0.989992440\n",
            "2076:\t-1.024220824\n",
            "2077:\t-1.016928196\n",
            "2078:\t-0.896759927\n",
            "2079:\t-1.006807566\n",
            "2080:\t-0.908662796\n",
            "2081:\t-0.935061336\n",
            "2082:\t-1.025910139\n",
            "2083:\t-0.988743424\n",
            "2084:\t-0.903758585\n",
            "2085:\t-1.052453160\n",
            "2086:\t-0.934977829\n",
            "2087:\t-0.919456184\n",
            "2088:\t-1.121304035\n",
            "2089:\t-0.919841409\n",
            "2090:\t-1.122684360\n",
            "2091:\t-1.018298388\n",
            "2092:\t-0.880575240\n",
            "2093:\t-1.074587941\n",
            "2094:\t-0.933851004\n",
            "2095:\t-1.133826733\n",
            "2096:\t-0.976113677\n",
            "2097:\t-0.920318246\n",
            "2098:\t-0.916344225\n",
            "2099:\t-0.980658650\n",
            "2100:\t-0.883704782\n",
            "2101:\t-0.884949863\n",
            "2102:\t-0.821350813\n",
            "2103:\t-0.929708600\n",
            "2104:\t-0.915562928\n",
            "2105:\t-1.130934119\n",
            "2106:\t-0.957176566\n",
            "2107:\t-0.943237662\n",
            "2108:\t-0.872215271\n",
            "2109:\t-0.896812379\n",
            "2110:\t-0.937556684\n",
            "2111:\t-0.884111524\n",
            "2112:\t-0.951112866\n",
            "2113:\t-1.005759239\n",
            "2114:\t-1.026369691\n",
            "2115:\t-0.952274740\n",
            "2116:\t-1.045771956\n",
            "2117:\t-1.084491134\n",
            "2118:\t-0.999683201\n",
            "2119:\t-0.890019000\n",
            "2120:\t-0.885850549\n",
            "2121:\t-1.062495351\n",
            "2122:\t-1.062319636\n",
            "2123:\t-0.916264951\n",
            "2124:\t-0.970716596\n",
            "2125:\t-0.895467520\n",
            "2126:\t-0.900143206\n",
            "2127:\t-0.890769958\n",
            "2128:\t-0.889612377\n",
            "2129:\t-0.927718878\n",
            "2130:\t-0.947756231\n",
            "2131:\t-0.902622461\n",
            "2132:\t-1.087428927\n",
            "2133:\t-0.929781020\n",
            "2134:\t-0.874535501\n",
            "2135:\t-0.920740843\n",
            "2136:\t-0.938418329\n",
            "2137:\t-1.126025796\n",
            "2138:\t-1.024909139\n",
            "2139:\t-1.032281876\n",
            "2140:\t-0.914611638\n",
            "2141:\t-0.972653925\n",
            "2142:\t-1.072380781\n",
            "2143:\t-1.089801669\n",
            "2144:\t-1.083711743\n",
            "2145:\t-1.056833506\n",
            "2146:\t-0.920601964\n",
            "2147:\t-1.111030698\n",
            "2148:\t-0.965951383\n",
            "2149:\t-0.873409629\n",
            "2150:\t-0.863885045\n",
            "2151:\t-1.042095900\n",
            "2152:\t-0.996411681\n",
            "2153:\t-0.936350524\n",
            "2154:\t-1.031835556\n",
            "2155:\t-0.873317659\n",
            "2156:\t-0.961467505\n",
            "2157:\t-0.902758539\n",
            "2158:\t-1.058058739\n",
            "2159:\t-0.927662134\n",
            "2160:\t-0.964101315\n",
            "2161:\t-1.105051994\n",
            "2162:\t-1.089593887\n",
            "2163:\t-0.927594602\n",
            "2164:\t-0.997759998\n",
            "2165:\t-0.913158655\n",
            "2166:\t-1.009524345\n",
            "2167:\t-0.923509181\n",
            "2168:\t-1.169963002\n",
            "2169:\t-1.075572133\n",
            "2170:\t-1.067242980\n",
            "2171:\t-0.994322121\n",
            "2172:\t-0.865945041\n",
            "2173:\t-0.893361688\n",
            "2174:\t-1.001014709\n",
            "2175:\t-0.999803185\n",
            "2176:\t-0.909751773\n",
            "2177:\t-1.022291422\n",
            "2178:\t-1.058233500\n",
            "2179:\t-0.995184124\n",
            "2180:\t-0.988335252\n",
            "2181:\t-0.972255886\n",
            "2182:\t-0.927066565\n",
            "2183:\t-0.914737344\n",
            "2184:\t-1.003224969\n",
            "2185:\t-1.119688153\n",
            "2186:\t-1.037172198\n",
            "2187:\t-1.117899537\n",
            "2188:\t-0.945969045\n",
            "2189:\t-0.920134783\n",
            "2190:\t-1.103759408\n",
            "2191:\t-0.828599334\n",
            "2192:\t-0.913522303\n",
            "2193:\t-1.030173898\n",
            "2194:\t-1.007384539\n",
            "2195:\t-0.955959558\n",
            "2196:\t-1.036685944\n",
            "2197:\t-0.992940664\n",
            "2198:\t-0.974946737\n",
            "2199:\t-0.945219934\n",
            "2200:\t-0.962339997\n",
            "2201:\t-1.039918900\n",
            "2202:\t-0.884609103\n",
            "2203:\t-1.022336602\n",
            "2204:\t-0.940462172\n",
            "2205:\t-1.028200865\n",
            "2206:\t-1.041830897\n",
            "2207:\t-0.908353448\n",
            "2208:\t-1.101940870\n",
            "2209:\t-1.084087849\n",
            "2210:\t-0.926495075\n",
            "2211:\t-1.096348882\n",
            "2212:\t-1.016984820\n",
            "2213:\t-0.913959622\n",
            "2214:\t-1.110668421\n",
            "2215:\t-0.874835253\n",
            "2216:\t-0.907122672\n",
            "2217:\t-1.042283893\n",
            "2218:\t-0.903816998\n",
            "2219:\t-1.087473750\n",
            "2220:\t-0.955627561\n",
            "2221:\t-0.915596366\n",
            "2222:\t-1.120262980\n",
            "2223:\t-0.928953350\n",
            "2224:\t-0.989111781\n",
            "2225:\t-0.884321988\n",
            "2226:\t-1.046273112\n",
            "2227:\t-1.061178684\n",
            "2228:\t-0.947538674\n",
            "2229:\t-0.887025177\n",
            "2230:\t-0.897700310\n",
            "2231:\t-1.056797862\n",
            "2232:\t-0.875148654\n",
            "2233:\t-0.885778666\n",
            "2234:\t-1.008702278\n",
            "2235:\t-0.917492807\n",
            "2236:\t-1.016134381\n",
            "2237:\t-1.076156497\n",
            "2238:\t-0.895981848\n",
            "2239:\t-0.989255965\n",
            "2240:\t-1.052419782\n",
            "2241:\t-0.931173682\n",
            "2242:\t-1.037038922\n",
            "2243:\t-0.876339614\n",
            "2244:\t-1.095780730\n",
            "2245:\t-0.958719194\n",
            "2246:\t-1.024355531\n",
            "2247:\t-1.075659633\n",
            "2248:\t-0.902948201\n",
            "2249:\t-0.892008424\n",
            "2250:\t-1.027812600\n",
            "2251:\t-1.097900629\n",
            "2252:\t-0.893926978\n",
            "2253:\t-0.889362872\n",
            "2254:\t-1.022040963\n",
            "2255:\t-1.115639329\n",
            "2256:\t-0.941735208\n",
            "2257:\t-0.897698283\n",
            "2258:\t-0.991396129\n",
            "2259:\t-1.013671875\n",
            "2260:\t-0.983385146\n",
            "2261:\t-0.896900356\n",
            "2262:\t-1.157300472\n",
            "2263:\t-1.029841065\n",
            "2264:\t-1.065358162\n",
            "2265:\t-0.845619917\n",
            "2266:\t-1.008477688\n",
            "2267:\t-0.874619305\n",
            "2268:\t-0.900164068\n",
            "2269:\t-0.880091906\n",
            "2270:\t-0.874018610\n",
            "2271:\t-0.966021895\n",
            "2272:\t-0.899025977\n",
            "2273:\t-0.942773759\n",
            "2274:\t-0.911646307\n",
            "2275:\t-0.943336368\n",
            "2276:\t-0.888518155\n",
            "2277:\t-0.918826938\n",
            "2278:\t-0.890778601\n",
            "2279:\t-0.992002726\n",
            "2280:\t-0.942191482\n",
            "2281:\t-0.922741175\n",
            "2282:\t-1.098271489\n",
            "2283:\t-1.071561217\n",
            "2284:\t-0.967213631\n",
            "2285:\t-0.903311670\n",
            "2286:\t-0.900361896\n",
            "2287:\t-1.090526342\n",
            "2288:\t-0.918097913\n",
            "2289:\t-1.070270538\n",
            "2290:\t-0.896647334\n",
            "2291:\t-0.923211634\n",
            "2292:\t-0.985726357\n",
            "2293:\t-0.907865882\n",
            "2294:\t-1.044040084\n",
            "2295:\t-0.887360036\n",
            "2296:\t-0.943257034\n",
            "2297:\t-0.948547542\n",
            "2298:\t-1.011438727\n",
            "2299:\t-1.069393158\n",
            "2300:\t-1.005133986\n",
            "2301:\t-0.897502363\n",
            "2302:\t-1.007901669\n",
            "2303:\t-0.905277908\n",
            "2304:\t-1.113636613\n",
            "2305:\t-0.906519532\n",
            "Epoch: 5\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "2306:\t-0.918017268\n",
            "2307:\t-1.005747080\n",
            "2308:\t-0.915655017\n",
            "2309:\t-1.094897270\n",
            "2310:\t-0.957080245\n",
            "2311:\t-1.033888102\n",
            "2312:\t-0.906566441\n",
            "2313:\t-0.939094603\n",
            "2314:\t-1.059626102\n",
            "2315:\t-0.985633969\n",
            "2316:\t-1.027003050\n",
            "2317:\t-0.955835938\n",
            "2318:\t-0.907290220\n",
            "2319:\t-1.006229401\n",
            "2320:\t-1.113212466\n",
            "2321:\t-0.885473013\n",
            "2322:\t-1.020224452\n",
            "2323:\t-0.975776970\n",
            "2324:\t-0.919342637\n",
            "2325:\t-1.022235274\n",
            "2326:\t-1.133092999\n",
            "2327:\t-0.924032032\n",
            "2328:\t-0.907350600\n",
            "2329:\t-0.905439079\n",
            "2330:\t-1.009109735\n",
            "2331:\t-1.037487864\n",
            "2332:\t-0.980386257\n",
            "2333:\t-0.917018116\n",
            "2334:\t-0.912848413\n",
            "2335:\t-0.986065269\n",
            "2336:\t-0.907143474\n",
            "2337:\t-0.967925549\n",
            "2338:\t-0.923049808\n",
            "2339:\t-0.991077065\n",
            "2340:\t-0.920207381\n",
            "2341:\t-1.143914580\n",
            "2342:\t-0.936204076\n",
            "2343:\t-1.016346335\n",
            "2344:\t-0.898583472\n",
            "2345:\t-0.914598584\n",
            "2346:\t-0.900063097\n",
            "2347:\t-1.096291184\n",
            "2348:\t-0.984236956\n",
            "2349:\t-1.038524389\n",
            "2350:\t-0.886123657\n",
            "2351:\t-0.976265728\n",
            "2352:\t-1.098163843\n",
            "2353:\t-1.073794484\n",
            "2354:\t-0.992747962\n",
            "2355:\t-0.903322041\n",
            "2356:\t-0.964731812\n",
            "2357:\t-0.918141127\n",
            "2358:\t-1.012604952\n",
            "2359:\t-0.895301640\n",
            "2360:\t-1.075860262\n",
            "2361:\t-0.993521035\n",
            "2362:\t-0.891141474\n",
            "2363:\t-1.076316714\n",
            "2364:\t-1.019704103\n",
            "2365:\t-0.909853160\n",
            "2366:\t-0.945946574\n",
            "2367:\t-0.997287035\n",
            "2368:\t-1.013547778\n",
            "2369:\t-1.053362727\n",
            "2370:\t-1.121619463\n",
            "2371:\t-1.023704886\n",
            "2372:\t-0.900077105\n",
            "2373:\t-0.924150229\n",
            "2374:\t-1.022215247\n",
            "2375:\t-0.997870743\n",
            "2376:\t-0.911637187\n",
            "2377:\t-1.051768064\n",
            "2378:\t-0.903562248\n",
            "2379:\t-0.903960109\n",
            "2380:\t-0.939412415\n",
            "2381:\t-0.943948627\n",
            "2382:\t-1.046980500\n",
            "2383:\t-0.923791766\n",
            "2384:\t-0.890681326\n",
            "2385:\t-1.143704772\n",
            "2386:\t-0.943867028\n",
            "2387:\t-1.106248021\n",
            "2388:\t-0.962498188\n",
            "2389:\t-0.891384840\n",
            "2390:\t-0.924437940\n",
            "2391:\t-1.080172777\n",
            "2392:\t-0.935701013\n",
            "2393:\t-1.002337337\n",
            "2394:\t-1.169674635\n",
            "2395:\t-1.007343531\n",
            "2396:\t-0.988712072\n",
            "2397:\t-1.044079661\n",
            "2398:\t-1.089999795\n",
            "2399:\t-1.039418101\n",
            "2400:\t-0.880159378\n",
            "2401:\t-0.967339754\n",
            "2402:\t-1.129864454\n",
            "2403:\t-0.890217841\n",
            "2404:\t-1.079282045\n",
            "2405:\t-0.870417237\n",
            "2406:\t-1.114484668\n",
            "2407:\t-0.916230202\n",
            "2408:\t-0.922542334\n",
            "2409:\t-0.907964528\n",
            "2410:\t-1.012986779\n",
            "2411:\t-0.979938924\n",
            "2412:\t-1.074893236\n",
            "2413:\t-1.030408382\n",
            "2414:\t-1.186484694\n",
            "2415:\t-0.965190709\n",
            "2416:\t-1.045117855\n",
            "2417:\t-0.920847654\n",
            "2418:\t-1.014726043\n",
            "2419:\t-0.910465658\n",
            "2420:\t-1.049755454\n",
            "2421:\t-0.995740533\n",
            "2422:\t-1.006236792\n",
            "2423:\t-0.901877940\n",
            "2424:\t-0.899449944\n",
            "2425:\t-0.985269606\n",
            "2426:\t-0.909228265\n",
            "2427:\t-0.979228854\n",
            "2428:\t-0.959511995\n",
            "2429:\t-1.054590821\n",
            "2430:\t-0.994217873\n",
            "2431:\t-0.928685427\n",
            "2432:\t-0.894222796\n",
            "2433:\t-1.124060988\n",
            "2434:\t-0.979677737\n",
            "2435:\t-1.105540991\n",
            "2436:\t-0.888810158\n",
            "2437:\t-1.012609124\n",
            "2438:\t-1.078110933\n",
            "2439:\t-1.006563902\n",
            "2440:\t-0.970031500\n",
            "2441:\t-1.189589739\n",
            "2442:\t-0.906415880\n",
            "2443:\t-0.988342166\n",
            "2444:\t-0.987861931\n",
            "2445:\t-0.996561229\n",
            "2446:\t-1.142476797\n",
            "2447:\t-0.898037970\n",
            "2448:\t-1.182213068\n",
            "2449:\t-0.933769822\n",
            "2450:\t-1.194243670\n",
            "2451:\t-0.889300525\n",
            "2452:\t-0.936710298\n",
            "2453:\t-1.025074959\n",
            "2454:\t-0.964325786\n",
            "2455:\t-1.013529420\n",
            "2456:\t-1.168193579\n",
            "2457:\t-0.982859850\n",
            "2458:\t-0.918682933\n",
            "2459:\t-0.901319027\n",
            "2460:\t-1.057091951\n",
            "2461:\t-0.936073899\n",
            "2462:\t-0.910151303\n",
            "2463:\t-1.065773606\n",
            "2464:\t-0.903220057\n",
            "2465:\t-1.052177787\n",
            "2466:\t-1.105848074\n",
            "2467:\t-0.880655706\n",
            "2468:\t-0.977753699\n",
            "2469:\t-1.016041636\n",
            "2470:\t-0.890644848\n",
            "2471:\t-1.083288550\n",
            "2472:\t-0.953381836\n",
            "2473:\t-1.053423166\n",
            "2474:\t-0.912175894\n",
            "2475:\t-1.044755697\n",
            "2476:\t-0.965073824\n",
            "2477:\t-1.071198940\n",
            "2478:\t-0.890233994\n",
            "2479:\t-1.091261029\n",
            "2480:\t-1.131519079\n",
            "2481:\t-0.899053633\n",
            "2482:\t-1.079938650\n",
            "2483:\t-1.024953127\n",
            "2484:\t-1.052369833\n",
            "2485:\t-0.995061636\n",
            "2486:\t-1.034809828\n",
            "2487:\t-1.152536750\n",
            "2488:\t-1.127593517\n",
            "2489:\t-0.894396663\n",
            "2490:\t-0.968808174\n",
            "2491:\t-0.913915217\n",
            "2492:\t-0.982288957\n",
            "2493:\t-0.900725067\n",
            "2494:\t-0.935566843\n",
            "2495:\t-0.903825939\n",
            "2496:\t-0.915378630\n",
            "2497:\t-1.210989475\n",
            "2498:\t-0.925104380\n",
            "2499:\t-1.098635793\n",
            "2500:\t-0.935203254\n",
            "Saving model and optimizer state at iteration 2500 to outdir/model_2500\n",
            "2501:\t-0.907427371\n",
            "2502:\t-0.918061018\n",
            "2503:\t-0.931608677\n",
            "2504:\t-1.005144835\n",
            "2505:\t-0.890618801\n",
            "2506:\t-1.035645247\n",
            "2507:\t-0.952966630\n",
            "2508:\t-0.927330375\n",
            "2509:\t-0.907984674\n",
            "2510:\t-0.896740556\n",
            "2511:\t-0.916212916\n",
            "2512:\t-0.993378043\n",
            "2513:\t-0.995997012\n",
            "2514:\t-0.908470750\n",
            "2515:\t-0.944839060\n",
            "2516:\t-0.902151406\n",
            "2517:\t-1.113395452\n",
            "2518:\t-0.918193102\n",
            "2519:\t-0.915379882\n",
            "2520:\t-0.903487861\n",
            "2521:\t-1.071673751\n",
            "2522:\t-0.991302013\n",
            "2523:\t-0.901736736\n",
            "2524:\t-0.944299757\n",
            "2525:\t-0.984685540\n",
            "2526:\t-0.985961080\n",
            "2527:\t-0.896266878\n",
            "2528:\t-0.888309300\n",
            "2529:\t-1.182145596\n",
            "2530:\t-0.955801666\n",
            "2531:\t-0.991273522\n",
            "2532:\t-0.911191046\n",
            "2533:\t-0.956119180\n",
            "2534:\t-1.041648030\n",
            "2535:\t-0.939822495\n",
            "2536:\t-1.073704243\n",
            "2537:\t-0.903157294\n",
            "2538:\t-1.023678660\n",
            "2539:\t-1.063852072\n",
            "2540:\t-0.898418009\n",
            "2541:\t-0.900801420\n",
            "2542:\t-0.898634374\n",
            "2543:\t-1.017111659\n",
            "2544:\t-0.985272765\n",
            "2545:\t-0.920512497\n",
            "2546:\t-0.930373907\n",
            "2547:\t-0.934437096\n",
            "2548:\t-0.963243783\n",
            "2549:\t-0.896879554\n",
            "2550:\t-0.994310021\n",
            "2551:\t-1.097990513\n",
            "2552:\t-1.045399070\n",
            "2553:\t-1.029394269\n",
            "2554:\t-1.044136047\n",
            "2555:\t-1.037991524\n",
            "2556:\t-1.085623622\n",
            "2557:\t-0.890751541\n",
            "2558:\t-0.902419865\n",
            "2559:\t-0.895753980\n",
            "2560:\t-0.942756236\n",
            "2561:\t-0.835749090\n",
            "2562:\t-1.114468575\n",
            "2563:\t-0.983218670\n",
            "2564:\t-0.902257204\n",
            "2565:\t-0.888746142\n",
            "2566:\t-1.057998776\n",
            "2567:\t-1.020435929\n",
            "2568:\t-1.087122679\n",
            "2569:\t-0.928222060\n",
            "2570:\t-0.949467957\n",
            "2571:\t-0.923998296\n",
            "2572:\t-0.904139996\n",
            "2573:\t-0.938490689\n",
            "2574:\t-0.892145693\n",
            "2575:\t-0.939606726\n",
            "2576:\t-1.109411001\n",
            "2577:\t-1.074740648\n",
            "2578:\t-0.895843148\n",
            "2579:\t-0.929115057\n",
            "2580:\t-0.920938790\n",
            "2581:\t-0.928393841\n",
            "2582:\t-0.991367161\n",
            "2583:\t-1.062691569\n",
            "2584:\t-0.920020640\n",
            "2585:\t-1.020414710\n",
            "2586:\t-0.894021988\n",
            "2587:\t-1.104117513\n",
            "2588:\t-0.915418565\n",
            "2589:\t-0.848257959\n",
            "2590:\t-1.015796185\n",
            "2591:\t-0.894853771\n",
            "2592:\t-0.873040140\n",
            "2593:\t-0.994228184\n",
            "2594:\t-1.065780282\n",
            "2595:\t-1.056097746\n",
            "2596:\t-1.091955781\n",
            "2597:\t-0.897591889\n",
            "2598:\t-0.918207645\n",
            "2599:\t-0.996976495\n",
            "2600:\t-0.998813510\n",
            "2601:\t-0.940603852\n",
            "2602:\t-0.923988342\n",
            "2603:\t-0.884848237\n",
            "2604:\t-1.095230579\n",
            "2605:\t-1.036752939\n",
            "2606:\t-1.126305342\n",
            "2607:\t-0.914478481\n",
            "2608:\t-0.891305029\n",
            "2609:\t-0.995103180\n",
            "2610:\t-0.987168729\n",
            "2611:\t-0.944192111\n",
            "2612:\t-1.022670627\n",
            "2613:\t-0.893912196\n",
            "2614:\t-1.014231443\n",
            "2615:\t-1.050189614\n",
            "2616:\t-1.075481415\n",
            "2617:\t-1.098963380\n",
            "2618:\t-1.011429906\n",
            "2619:\t-0.913703024\n",
            "2620:\t-1.174491644\n",
            "2621:\t-0.882492065\n",
            "2622:\t-0.910436273\n",
            "2623:\t-0.857366920\n",
            "2624:\t-1.003565311\n",
            "2625:\t-0.902781606\n",
            "2626:\t-0.771097362\n",
            "2627:\t-0.931056380\n",
            "2628:\t-0.891409338\n",
            "2629:\t-0.895421267\n",
            "2630:\t-0.944613934\n",
            "2631:\t-0.957752407\n",
            "2632:\t-0.947762012\n",
            "2633:\t-0.893011689\n",
            "2634:\t-0.972379506\n",
            "2635:\t-0.881808400\n",
            "2636:\t-0.905596018\n",
            "2637:\t-0.866099536\n",
            "2638:\t-0.982591450\n",
            "2639:\t-0.952099741\n",
            "2640:\t-0.909525514\n",
            "2641:\t-1.028306127\n",
            "2642:\t-0.909350216\n",
            "2643:\t-0.896366060\n",
            "2644:\t-0.945051372\n",
            "2645:\t-0.956629157\n",
            "2646:\t-0.924042344\n",
            "2647:\t-0.944141269\n",
            "2648:\t-0.969904840\n",
            "2649:\t-0.929057002\n",
            "2650:\t-0.953973532\n",
            "2651:\t-0.932044566\n",
            "2652:\t-0.884639502\n",
            "2653:\t-0.975666940\n",
            "2654:\t-1.039948344\n",
            "2655:\t-0.958160818\n",
            "2656:\t-0.969803214\n",
            "2657:\t-0.863113940\n",
            "2658:\t-0.937095761\n",
            "2659:\t-1.053032398\n",
            "2660:\t-0.890185058\n",
            "2661:\t-0.897324443\n",
            "2662:\t-0.966284871\n",
            "2663:\t-0.968026638\n",
            "2664:\t-0.890403986\n",
            "2665:\t-0.960123479\n",
            "2666:\t-0.966202199\n",
            "2667:\t-1.019959450\n",
            "2668:\t-0.955639243\n",
            "2669:\t-1.053770781\n",
            "2670:\t-0.956020653\n",
            "2671:\t-0.949489653\n",
            "2672:\t-0.928022861\n",
            "2673:\t-0.895987749\n",
            "2674:\t-1.064576507\n",
            "2675:\t-0.983593941\n",
            "2676:\t-0.999670088\n",
            "2677:\t-0.921035945\n",
            "2678:\t-0.968020439\n",
            "2679:\t-0.944008529\n",
            "2680:\t-0.917200267\n",
            "2681:\t-0.974401236\n",
            "2682:\t-1.005730271\n",
            "2683:\t-0.906632841\n",
            "2684:\t-0.936517358\n",
            "2685:\t-1.055845618\n",
            "2686:\t-1.042393565\n",
            "2687:\t-0.984707892\n",
            "2688:\t-1.014071941\n",
            "2689:\t-1.044359446\n",
            "2690:\t-1.070381999\n",
            "2691:\t-0.910096109\n",
            "2692:\t-0.906313241\n",
            "2693:\t-0.921419501\n",
            "2694:\t-1.044924855\n",
            "2695:\t-1.092303395\n",
            "2696:\t-0.942739964\n",
            "2697:\t-1.071466446\n",
            "2698:\t-0.889463961\n",
            "2699:\t-0.983120441\n",
            "2700:\t-0.912289083\n",
            "2701:\t-1.089798331\n",
            "2702:\t-1.054820299\n",
            "2703:\t-0.938423455\n",
            "2704:\t-1.029830337\n",
            "2705:\t-1.080193162\n",
            "2706:\t-0.922187746\n",
            "2707:\t-0.898149967\n",
            "2708:\t-0.895202577\n",
            "2709:\t-1.167059660\n",
            "2710:\t-0.884532690\n",
            "2711:\t-0.921231508\n",
            "2712:\t-0.890796304\n",
            "2713:\t-0.957283199\n",
            "2714:\t-0.927338362\n",
            "2715:\t-1.147615671\n",
            "2716:\t-1.007192373\n",
            "2717:\t-0.927119195\n",
            "2718:\t-1.004190326\n",
            "2719:\t-0.921182454\n",
            "2720:\t-1.041885257\n",
            "2721:\t-0.934199810\n",
            "2722:\t-1.001715899\n",
            "2723:\t-0.887319684\n",
            "2724:\t-1.134353399\n",
            "2725:\t-1.100150108\n",
            "2726:\t-1.054585099\n",
            "2727:\t-1.021649361\n",
            "2728:\t-0.876739740\n",
            "2729:\t-1.043360472\n",
            "2730:\t-0.982742965\n",
            "2731:\t-1.097524881\n",
            "2732:\t-1.023561597\n",
            "2733:\t-0.950580418\n",
            "2734:\t-0.949258924\n",
            "2735:\t-0.936707556\n",
            "2736:\t-1.045922518\n",
            "2737:\t-1.167978764\n",
            "2738:\t-1.029455185\n",
            "2739:\t-0.954377115\n",
            "2740:\t-0.906958699\n",
            "2741:\t-0.890752435\n",
            "2742:\t-0.907099485\n",
            "2743:\t-0.908097625\n",
            "2744:\t-0.991556346\n",
            "2745:\t-1.067684531\n",
            "2746:\t-0.997941554\n",
            "2747:\t-1.055665255\n",
            "2748:\t-0.939157665\n",
            "2749:\t-0.905731082\n",
            "2750:\t-0.957349658\n",
            "2751:\t-1.064056039\n",
            "2752:\t-0.995085955\n",
            "2753:\t-0.912572145\n",
            "2754:\t-1.076983809\n",
            "2755:\t-0.889528155\n",
            "2756:\t-0.901882112\n",
            "2757:\t-0.899109662\n",
            "2758:\t-0.918298662\n",
            "2759:\t-1.194654107\n",
            "2760:\t-0.917607486\n",
            "2761:\t-1.004058719\n",
            "2762:\t-0.919922113\n",
            "2763:\t-0.901509523\n",
            "2764:\t-0.998782694\n",
            "2765:\t-0.913291276\n",
            "2766:\t-1.102415800\n",
            "Epoch: 6\n",
            "/content/flowtron/data.py:40: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.from_numpy(data).float(), sampling_rate\n",
            "2767:\t-0.921156228\n",
            "2768:\t-0.966874838\n",
            "2769:\t-1.034394145\n",
            "2770:\t-1.065991402\n",
            "2771:\t-0.999771535\n",
            "2772:\t-0.964870572\n",
            "2773:\t-1.036896706\n",
            "2774:\t-1.017906070\n",
            "2775:\t-0.910062253\n",
            "2776:\t-0.918047726\n",
            "2777:\t-0.984932542\n",
            "2778:\t-1.116407275\n",
            "2779:\t-1.150009751\n",
            "2780:\t-0.958223701\n",
            "2781:\t-1.087767243\n",
            "2782:\t-1.061148167\n",
            "2783:\t-0.954193354\n",
            "2784:\t-1.002560854\n",
            "2785:\t-1.109157801\n",
            "2786:\t-0.891084254\n",
            "2787:\t-1.068692446\n",
            "2788:\t-0.957086265\n",
            "2789:\t-0.935117245\n",
            "2790:\t-1.121032834\n",
            "2791:\t-1.087188244\n",
            "2792:\t-0.926874757\n",
            "2793:\t-1.057372808\n",
            "2794:\t-0.908531487\n",
            "2795:\t-1.123865724\n",
            "2796:\t-0.898898661\n",
            "2797:\t-0.993851364\n",
            "2798:\t-0.890176892\n",
            "2799:\t-1.003520966\n",
            "2800:\t-0.913273513\n",
            "2801:\t-1.121461749\n",
            "2802:\t-0.936838686\n",
            "2803:\t-1.077359915\n",
            "2804:\t-0.989934802\n",
            "2805:\t-1.022138119\n",
            "2806:\t-0.918308854\n",
            "2807:\t-1.043413401\n",
            "2808:\t-1.083362699\n",
            "2809:\t-0.951490521\n",
            "2810:\t-1.046215773\n",
            "2811:\t-0.957181096\n",
            "2812:\t-0.944145858\n",
            "2813:\t-0.867891908\n",
            "2814:\t-1.035352111\n",
            "2815:\t-0.902497053\n",
            "2816:\t-1.015503645\n",
            "2817:\t-1.040462494\n",
            "2818:\t-0.968107045\n",
            "2819:\t-1.092110276\n",
            "2820:\t-0.920104563\n",
            "2821:\t-0.915906131\n",
            "2822:\t-1.138422966\n",
            "2823:\t-0.954476416\n",
            "2824:\t-0.989368200\n",
            "2825:\t-0.902794242\n",
            "2826:\t-0.934892058\n",
            "2827:\t-1.156048298\n",
            "2828:\t-0.934274495\n",
            "2829:\t-0.964352846\n",
            "2830:\t-0.900512457\n",
            "2831:\t-0.887019455\n",
            "2832:\t-0.914312243\n",
            "2833:\t-1.046504617\n",
            "2834:\t-0.882045269\n",
            "2835:\t-0.893199623\n",
            "2836:\t-1.040963650\n",
            "2837:\t-1.034978271\n",
            "2838:\t-0.917794466\n",
            "2839:\t-0.988251626\n",
            "2840:\t-0.907049239\n",
            "2841:\t-0.940917373\n",
            "2842:\t-1.070912123\n",
            "2843:\t-0.931381583\n",
            "2844:\t-1.057648420\n",
            "2845:\t-0.962784648\n",
            "2846:\t-0.935428619\n",
            "2847:\t-1.010089874\n",
            "2848:\t-1.078867555\n",
            "2849:\t-0.982297599\n",
            "2850:\t-1.092441440\n",
            "2851:\t-1.107310414\n",
            "2852:\t-0.908527792\n",
            "2853:\t-0.979867578\n",
            "2854:\t-0.895451546\n",
            "2855:\t-1.051684737\n",
            "2856:\t-0.859886467\n",
            "2857:\t-0.905815840\n",
            "2858:\t-0.951348543\n",
            "2859:\t-1.013306499\n",
            "2860:\t-0.999455214\n",
            "2861:\t-1.005122542\n",
            "2862:\t-0.915235102\n",
            "2863:\t-0.899264812\n",
            "2864:\t-1.147344232\n",
            "2865:\t-0.911981165\n",
            "2866:\t-0.999223590\n",
            "2867:\t-1.063754797\n",
            "2868:\t-1.063250661\n",
            "2869:\t-1.096031785\n",
            "2870:\t-0.911615729\n",
            "2871:\t-0.962343812\n",
            "2872:\t-0.997008026\n",
            "2873:\t-0.875193477\n",
            "2874:\t-0.957424045\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/flowtron/train.py\", line 309, in <module>\n",
            "    train(n_gpus, rank, **train_config)\n",
            "  File \"/content/flowtron/train.py\", line 247, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/tensor.py\", line 185, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\", line 127, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "RuntimeError: CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 15.90 GiB total capacity; 10.16 GiB already allocated; 2.48 GiB free; 12.63 GiB reserved in total by PyTorch)\n",
            "Exception raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\n",
            "frame #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f64292df1e2 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: <unknown function> + 0x1e64b (0x7f642953564b in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10_cuda.so)\n",
            "frame #2: <unknown function> + 0x1f464 (0x7f6429536464 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10_cuda.so)\n",
            "frame #3: <unknown function> + 0x1faa1 (0x7f6429536aa1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10_cuda.so)\n",
            "frame #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f642c24190e in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #5: <unknown function> + 0xf33949 (0x7f642a67b949 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #6: <unknown function> + 0xf4d777 (0x7f642a695777 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cuda.so)\n",
            "frame #7: <unknown function> + 0x10e9c7d (0x7f6465431c7d in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #8: <unknown function> + 0x10e9f97 (0x7f6465431f97 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f646553ca1a in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #10: at::TensorIterator::fast_set_up(at::TensorIteratorConfig const&) + 0x56a (0x7f64651c9f8a in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #11: at::TensorIterator::build(at::TensorIteratorConfig&) + 0x76 (0x7f64651cdff6 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #12: at::TensorIterator::TensorIterator(at::TensorIteratorConfig&) + 0xdd (0x7f64651ce65d in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #13: at::TensorIterator::binary_op(at::Tensor&, at::Tensor const&, at::Tensor const&, bool) + 0x14a (0x7f64651ce80a in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #14: at::native::tanh_backward(at::Tensor const&, at::Tensor const&) + 0x47 (0x7f6464f0c3b7 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #15: <unknown function> + 0x12a5380 (0x7f64655ed380 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #16: <unknown function> + 0xa56530 (0x7f6464d9e530 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f646558681c in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #18: at::tanh_backward(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f64654d956b in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #19: <unknown function> + 0x2ed7bdf (0x7f646721fbdf in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #20: <unknown function> + 0xa56530 (0x7f6464d9e530 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #21: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f646558681c in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #22: at::tanh_backward(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f64654d956b in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #23: torch::autograd::generated::TanhBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x185 (0x7f64670812c5 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #24: <unknown function> + 0x3375bb7 (0x7f64676bdbb7 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #25: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f64676b9400 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #26: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f64676b9fa1 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #27: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f64676b2119 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #28: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f6474e52dea in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #29: <unknown function> + 0xbd6df (0x7f64778546df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\n",
            "frame #30: <unknown function> + 0x76db (0x7f6478d276db in /lib/x86_64-linux-gnu/libpthread.so.0)\n",
            "frame #31: clone + 0x3f (0x7f6477e5c71f in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r96Dm7H5rFXJ"
      },
      "source": [
        "# Load Tensorboard to view training metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Crp6cXfq_My"
      },
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir /content/flowtron/outdir/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeNzlRZurJ76"
      },
      "source": [
        "# Inference\n",
        "Alter the text for any input text, Make sure you alter the model file to the step count you would like to test. eg content/flowtron/outdir/model_2500 for the checkpoint at 2500 steps. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8IVl6NTJo6B",
        "outputId": "e9660d40-dca6-4810-dcf7-8c55cc01faf3"
      },
      "source": [
        "!python inference.py -c config.json -f /content/flowtron/outdir/model_500 -w /content/flowtron/waveglow_256channels_v4.pt -t \"How are you?\" -i 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'inference.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}